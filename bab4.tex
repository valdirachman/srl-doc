%!TEX root = skripsi.tex
%-----------------------------------------------------------------------------%
\chapter{\babEmpat} \label{eksperimen}
%-----------------------------------------------------------------------------%

This chapter explains the implementations of the methodology explained in chapter 3. It includes the implementation of data annotation and pre-processing, model development, experiment, as well as the evaluation.

\section{Computer Specification}
For every experiment, we use GPU-based virtual server provided by Kata.ai. The specifications of the server are explained as follows.

\begin{table}
	\centering
	\caption{Server Specifications}
	\begin{tabular}{|l|l|}
		\hline
		\textbf{Processor} & i7-4770S \\ \hline
		\textbf{Number of Cores} & 8 core \\ \hline
		\textbf{Processor Frequency} & 3.1 GHz per core \\ \hline
		\textbf{RAM} & 8 GB \\ \hline
		\textbf{Operating System} & Ubuntu 14 \\ \hline
	\end{tabular}
	\label{table:spesifikasi hardware}
\end{table}

The server uses 8-core i7 processor with 3.1 GHz per core frequency. The size of the RAM is 8 GB. We use Ubuntu 14 distribution as the operating system.

\section{Data Annotation and Pre-processing}
For data annotation, we use an in-house tool provided by Kata.ai, named \textit{kata-annotator}. The total amount of data to be annotated was 9000 sentences. The data was annotated by three linguists with each of them annotating different set of data containing 3000 sentences for 8 weeks. In order to align the annotation understanding between them, the three linguists annotated the same trial set consisting of 100 sentences before starting to annotate the real one. The annotation differences found are then discussed in order to align the understanding between them.

After 8 weeks of annotation, the total amount of data that has been annotated is 8000 sentences. The other 1000 sentences missing is due to one annotator that could not complete the annotation on time. After finish annotating, the tool outputs the tokenized annotation result as JSON in BIO format. An example is given as follows:

\fbox{%
	\parbox{1.0\linewidth}{%
		[\\
			\{\\
				"data": ["Aku", "pengen", "makan", "ayam", "goreng", "dong"],\\
				"label": ["B-AGENT", "B-MD", "B-PRED", "B-PATIENT", "I-PATIENT", "O"]\\
			\},\\
			\{\\
				"data": ["Kamu", "gak", "tidur", ",", "Andi", "?"],\\
				"label": ["B-AGENT", "O", "B-PRED", "O", "B-GREET", "O"]\\
			\}\\
		]
	}%
}
\\

The label is then converted into a one-hot-vector representation which is presented in Pseudocode~\ref{code:labeltoone}
\begin{kode}
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	
	\SetKwProg{Fn}{Function}{ is}{end}
	\Fn{convertLabelToOneHotVector(arrLabel)}{
		\Input{array of labels of a sentence}
		\Output{array of one hot vectors}
		\BlankLine
		
		oneHotVectorLabel = []\;
		\ForEach{label in arrLabel}{
			oneHotVectorLabel.append(label.convertToOneHotVector())
		}
		\BlankLine
		
		\Return oneHotVectorLabel;
	}
	
	\caption{A pseudocode for converting labels of a sentence into one-hot-vectors}
	\label{code:labeltoone}
	
\end{kode}

It turns out there are only 5000 sentences which contain predicate in it. These 5000 sentences are the main data set to be trained and tested.

\section{Model Development}
This section explains the implementation of model development, including the feature extraction and model architecture. We use Python as our main programming language for all implementations.

\subsection{Feature Extraction}
The features to be extracted are word embedding, POS tag, and neighboring word embedding.

\subsubsection{Word Embedding}
% Jelasin pake gensim's word2vec. Parameter nya apa aja. Data training yang digunakan darimana.
We use Gensim's Word2Vec as the library for training the word embedding model as well as converting words into vectors. Pseudocode~\ref{code:trainword2ve} explains on how to train the word embedding model.
\begin{kode}
	
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\SetKwProg{Fn}{Function}{ is}{end}
	\Fn{trainWordEmbeddingModel(corpus, contextWindow, vectorDimension)}{
		\Input{training corpus, context window, vector dimension}
		\Output{Word2Vec model}
		
		\BlankLine
		model = Word2Vec.createModel(corpus, contextWindow, vectorDimension)
		
		\BlankLine
		\Return model;
	}
	
	\caption{A pseudocode to train word embedding model using Word2Vec}
	\label{code:trainword2ve}
\end{kode}

There are two parameters, which are context window and vector dimension. Context window determines the area of interest in building the word embedding model. Vector dimension represents the length of the output vector. In this work, the context window and vector dimension used are 5 and 32, respectively.

\begin{kode}
	
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\SetKwProg{Fn}{Function}{ is}{end}
	\Fn{wordToVector(model, arrWord)}{
		\Input{trained word embedding model, array of words of a sentence}
		\Output{array of word vectors}
		
		\BlankLine
		arrVector = []\;
		\ForEach{word in arrWord}{
			arrVector.append(model.getVector(word))
		}
		
		\BlankLine
		\Return arrVector;
	}
	
	\caption{A pseudocode to transform words into vectors by word embedding model}
	\label{code:ekstraksiownword}
\end{kode}
% jelasin kode convert word embedding
% jelasin kode train word embedding
From the Pseudocode~\ref{code:trainword2ve}, the output is the trained word embedding model. \ref{code:ekstraksiownword} shows how to use the model to convert words into vectors.

\subsubsection{POS Tag}
For POS Tag feature, we use the gold-standard POS tag annotated by the three linguists. The annotation tool \textit{kata-annotator} is also used for annotating the POS tag. The output example of the POS tag from the tool in a form of JSON is given as follows:

\fbox{%
	\parbox{1.0\linewidth}{%
		[\\
		\{\\
		"data": ["Aku", "pengen", "makan", "ayam", "goreng", "dong"],\\
		"label": ["NN", "ADV", "V", "NN", "V", "INTJ"]\\
		\},\\
		\{\\
		"data": ["Kamu", "gak", "tidur", ",", "Andi", "?"],\\
		"label": ["NN", "NEG", "V", "O", "NN", "O"]\\
		\}\\
		]
	}%
}
\\

% jelasin sedikit tentang contoh output tools nya
The JSON file consists of an array of sentences. Each word in a sentence is labeled with the POS tag accordingly.

\begin{kode}
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	
	\SetKwProg{Fn}{Function}{ is}{end}
	\Fn{convertPOSTagToOneHotVector(arrPOS)}{
		\Input{array of POS tags of a sentence}
		\Output{array of one hot vector}
		\BlankLine
		
		posTagFeature = []\;
		\ForEach{pos in arrPOS}{
			posTagFeature.append(pos.convertToOneHotVector())
		}
		\BlankLine
		
		\Return posTagFeature;
	}
	
	\caption{A pseudocode for converting POS tags of a sentence into one hot vectors}
	\label{code:ekstraksipostag}
\end{kode}

% jelasin sedikit tentang convert POS tag into one hot vector
Each of the POS tag in a sentence is then converted into one-hot-vector. The implementation is presented in Pseudocode~\ref{code:ekstraksipostag}.

\subsubsection{Neighboring Word Embeddings}
% jelasin ada dua, yaitu extract 1 word vector before and after. Pada sebelum token pertama dan setelah token terakhir ditambah padding vektor 0.
For neighboring word embeddings, we extract one vector on the left and one on the right of the word being processed. Pseudocode~\ref{code:neighboringwe} shows the implementation of extracting neighboring word embeddings.

\begin{kode}	
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	
	\SetAlgoLined
	\SetKwProg{Fn}{Function}{ is}{end}
	\Fn{extractNeighboringWordEmbedding(sentenceVector)}{
		\Input{array of word embedding vectors of a sentence}
		\Output{array of neighboring word embedding vectors}
		\BlankLine
		
		window = 1
		vectorDimension = getVectorDimension(sentenceVector)\;
		padded = window * [vectorDimension*[0.]] + sequence + window * [vectorDimension*[0.]]\;
		\BlankLine
		
		neighboringVectors = []\;
		\For{i in 0...sentenceVector.length - 1}{
			left = [item for sublist in padded[i:(i + window)] for item in sublist]\;
			right = [item for sublist in padded[(i+ window + 1):(i + nbContexts)] for item in sublist]\;
			concate = left + right\;
			neighboringVectors.append(concate)\;
		}
		\BlankLine
		
		\Return neighboringVectors;
		
	}	
	\caption{A pseudocode to extract neighboring word embeddings}
	\label{code:neighboringwe}	
\end{kode}

The parameter here is the window. Since we only extract one word on the left and another on the right of the word being processed, the value of parameter window is 1.

\subsection{Model Architecture}
As explained in chapter 3, we experiment on four model architectures, namely vanilla LSTM (LSTM), Bi-Directional LSTM (BLSTM), CNN-BLSTM, and Context-Aware BLSTM (CA-BLSTM). In this section, each implementation of the model architecture is explained. We use Keras 2.0~\citep{chollet2015} as our deep learning library with Tensorflow 1.0 backend for all the architectures. We use the \textit{functional model} of Keras. Keras model only receives input data with a fixed number of time steps for all sentences. Suppose that the maximum number of time steps in our data is $l$. Thus, sentences in our data whose number of time steps is lower than $l$ have to be padded with vector $ \vec{0}$ in order to get a fixed number of time steps of $l$. To do the padding, we use \textit{padsequences} function available from Keras.

\subsubsection{Vanilla LSTM (LSTM)}
Vanilla LSTM (LSTM) consists of only one layer of forward LSTM. Pseudocode~\ref{olstm} shows the implementation of the LSTM architecture.

\begin{kode}
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}

	\SetKwProg{Fn}{Function}{ is}{end}
	\Fn{lstm(xTrain, yTrain, timesteps, features, xTest, yTest)}{
		\Input{x train, y train, number of time steps, number of features, x test, y test}
		\Output{trained model, testing prediction result}
		\BlankLine
		
		inputLayer = Input(shape=(timesteps, features))\;
		forwardLayer = LSTM(units=128, returnSequences=True, recurrentDropout=0.2)(inputLayer)\;
		dropoutLayer = Dropout(0.2)(forwardLayer)\;
		outputLayer = TimeDistributed(Dense(units=17, activation='softmax'))(dropoutLayer)\;
		model = Model(inputs=[inputLayer], outputs=[outputLayer])\;
		\BlankLine
		
		model.compile(loss='categoricalCrossentropy', optimizer='adam')\;
		\BlankLine
		
		model.fit(xTrain, yTrain, epochs=50, batchSize=50)\;		
		prediction = model.predict(xTest)\;
		\BlankLine
		
		\Return model, prediction\;
	}
	
	\caption{A pseudocode for building and training vanilla LSTM architecture}
	\label{code:olstm}
\end{kode}

The Pseudocode~\ref{code:olstm} takes x train, y train, number of time steps, number of features, x test and y test as the inputs. The model starts with the defining the input layer, with the input shape of (timesteps, features). The input layer is then connected to the LSTM layer thas has 128 hidden units. These hidden units are the output of the LSTM layer. We use recurrent dropout in LSTM, as recommended by~\cite{he2017deep}. The recurrent dropout used is 0.2. We also use dropout layer on top of the LSTM layer by the value of 0.2. The output of the dropout layer is connected to the time-distributed dense layer with softmax activation function. These last layer produces the labels of semantic roles. The model is trained with categorical crossentropy loss function and Adam optimizer. The number of epochs and batch size used are both 50. After the model has been trained, we use it to predict the semantic roles the x test data which later will be evaluated. The function returns the trained model as well as the prediction result of the test data.

\subsubsection{Bi-Directional LSTM (BLSTM)}
Bi-Directional LSTM (BLSTM) consists of two layers of LSTM. The first layer is moving forward and the other is moving backward. Pseudocode~\ref{code:blstm} shows the implementation of the BLSTM architecture.

\begin{kode}
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	
	\SetKwProg{Fn}{Function}{ is}{end}
	\Fn{blstm(xTrain, yTrain, timesteps, features, xTest, yTest)}{
		\Input{x train, y train, number of time steps, number of features, x test, y test}
		\Output{trained model, testing prediction result}
		\BlankLine
		
		inputLayer = Input(shape=(timesteps, features))\;
		forwardLayer = LSTM(units=128, returnSequences=True, recurrentDropout=0.2)(inputLayer)\;
		backwardLayer = LSTM(units=128, returnSequences=True, goBackwards=True, recurrentDropout=0.2)(inputLayer)\;
		mergedLayer = Concatenate([forwardLayer, backwardLayer])\;
		dropoutLayer = Dropout(0.2)(mergedLayer)\;
		outputLayer = TimeDistributed(Dense(units=17, activation='softmax'))(dropoutLayer)\;
		model = Model(inputs=[inputLayer], outputs=[outputLayer])\;
		\BlankLine
		
		model.compile(loss='categoricalCrossentropy', optimizer='adam')\;
		\BlankLine
		
		model.fit(xTrain, yTrain, epochs=50, batchSize=50)\;		
		prediction = model.predict(xTest)\;
		\BlankLine
		
		\Return model, prediction\;
	}
	
	\caption{A pseudocode for building and training BLSTM architecture}
	\label{code:blstm}
\end{kode}

The Pseudocode~\ref{code:blstm} takes x train, y train, number of time steps, number of features, x test and y test as the inputs. The model starts with the defining the input layer, with the input shape of (timesteps, features). The input layer is then connected to two LSTM layers: forward layer and backward layer. Both forward and backward LSTM layers have 128 hidden units. The recurrent dropout used is 0.2. The output of both forward and backward layers are concatenated, resulting a vector whose length is 256. We also use dropout layer on top of the concatenated layer by the value of 0.2. The output of the dropout layer is connected to the time-distributed dense layer with softmax activation function. This last layer produces the labels of semantic roles. The model is trained with categorical crossentropy loss function and Adam optimizer. The number of epochs and batch size used are both 50. After the model has been trained, we use it to predict the semantic roles the x test data which later will be evaluated. The function returns the trained model as well as the prediction result of the test data.

\subsubsection{CNN-BLSTM}
CNN-BLSTM adds a CNN layer underneath the BLSTM layers. Pseudocode~\ref{code:cnnblstm} shows the implementation of the CNN-BLSTM architecture.
\begin{kode}
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	
	\SetKwProg{Fn}{Function}{ is}{end}
	\Fn{cnnblstm(xTrain, yTrain, timesteps, features, xTest, yTest)}{
		\Input{x train, y train, number of time steps, number of features, x test, y test}
		\Output{trained model, testing prediction result}
		\BlankLine
		
		inputLayer = Input(shape=(timesteps, features))\;
		cnnLayer = Conv1D(filters=128, kernelSize=3, padding='same', activation='relu', strides=1)(inputLayer)\;
		forwardLayer = LSTM(units=128, returnSequences=True, recurrentDropout=0.2)(cnnLayer)\;
		backwardLayer = LSTM(units=128, returnSequences=True, goBackwards=True, recurrentDropout=0.2)(cnnLayer)\;
		mergedLayer = Concatenate([forwardLayer, backwardLayer])\;
		dropoutLayer = Dropout(0.2)(mergedLayer)\;
		outputLayer = TimeDistributed(Dense(units=17, activation='softmax'))(dropoutLayer)\;
		model = Model(inputs=[inputLayer], outputs=[outputLayer])\;
		\BlankLine
		
		model.compile(loss='categoricalCrossentropy', optimizer='adam')\;
		\BlankLine
		
		model.fit(xTrain, yTrain, epochs=50, batchSize=50)\;
		prediction = model.predict(xTest)\;
		\BlankLine
		
		\Return model, prediction;
	}
	
	\caption{A pseudocode for building and training CNN-BLSTM architecture}
	\label{code:cnnblstm}
\end{kode}

In this architecture, the CNN layer is added before the BLSTM layers. Thus, the input layer is connected with the CNN layer. The parameters of the CNN layer are filters, kernel size, and strides. Filters represent the number of output hidden layers. Kernel size defines the context window of the CNN layer. Strides define the amount of slide for every time step. The value of filters, kernel size, and strides parameters are 128, 3, and 1, respectively. CNN layer is then connected to the BLSTM layers as explained in the previous section.

\subsubsection{Context-Aware BLSTM (CA-BLSTM)}
CA-BLSTM adds an attention mechanism on top of the BLSTM layers. Pseudocode~\ref{code:cablstm} shows the implementation of the CA-BLSTM architecture.
\begin{kode}
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	
	\SetKwProg{Fn}{Function}{ is}{end}
	\Fn{cablstm(xTrain, yTrain, timesteps, features, xTest, yTest)}{
		\Input{x train, y train, number of time steps, number of features, x test, y test}
		\Output{trained model, testing prediction result}
		\BlankLine
		
		inputLayer = Input(shape=(timesteps, features))\;
		cnnLayer = Conv1D(filters=filters, kernelSize=3, padding='same', activation='relu', strides=128)(inputLayer)\;
		forwardLayer = LSTM(units=128, returnSequences=True, recurrentDropout=0.2)(cnnLayer)\;
		backwardLayer = LSTM(units=128, returnSequences=True, goBackwards=True, recurrentDropout=0.2)(cnnLayer)\;
		mergedLayer = Concatenate([forwardLayer, backwardLayer])\;
		dropoutLayer = Dropout(0.2)(mergedLayer)\;
		\BlankLine
		
		outs1 = TimeDistributed(RawDense(outputDim=512))(dropoutLayer)\;
		m = Lambda(sum)(outs1)\;
		alpha = Dense(timesteps, activation="softmax")(m)\;
		z = Lambda(kaliAlphaSum)([alpha, dropoutLayer])\;
		dropoutZ = Dropout(0.2)(z)\;
		repeatedZ = RepeatVector(timesteps)(dropoutZ)\;
		outFinal = concatenate([dropoutLayer, repeatedZ])\;	
		outputLayer = TimeDistributed(Dense(units=17, activation='softmax'))(outFinal)\;
		model = Model(inputs=[inputLayer], outputs=[outputLayer])\;
		\BlankLine
		
		model.compile(loss='categoricalCrossentropy', optimizer='adam')\;
		\BlankLine
		
		model.fit(xTrain, yTrain, epochs=50, batchSize=50)\;
		prediction = model.predict(xTest)\;
		\BlankLine
		
		\Return model, prediction;
	}
	
	\caption{A pseudocode for building and training CA-BLSTM architecture}
	\label{code:cablstm}
\end{kode}

The output of the dropout layer from BLSTM is then fed into a time-distributed, raw dense layer. The output dimension of this dense layer is set to 512. The output of this layer is then summed for each time step. The result of this sum is then fed into the dense layer with softmax activation function. This dense layer outputs the weights alpha for each time step. Lambda function \textit{kaliAlphaSum} firstly multiplies the original dropout output of the BLSTM with the alpha values. The function then sums all the result element-wise in order to get a context vector z. Vector z is then duplicated by the number of time steps before it is concatenated with all the original dropout output of the BLSTM layers. These concatenated vectors are then fed into the last softmax layer to produce the output labels.

\section{Experiment}

\section{Evaluation}

%\section{Eksperimen}
%Pada tahap ini \saya~melakukan eksperimen model yang dikembangkan pada tahap sebelumnya. Sebelum masuk ke tahap eksperimen, \saya~melakukan beberapa tahap pra-eksperimen seperti melakukan pemecahan data sebagai implementasi \textit{cross-fold validation}. \Saya~memecah data menjadi 10 bagian dan disimpan dalam sebuah \textit{array} untuk masing-masing fitur. Berikut merupakan \textit{pseudocode} untuk melakukan pemecahan data
%
%\begin{kode}
%
%	\SetKwInOut{Input}{Input}
%	\SetKwInOut{Output}{Output}
%	
%	\SetKwProg{Fn}{Function}{ is}{end}
%	\Fn{splitting(featureArr)}{
%		\Input{array of feature}
%		\Output{splitted array of feature}
%		\BlankLine
%		
%		lenSplit = len(featureArr)/10\;
%		arrSplitted = []\;
%		\For{i=0; i<10;i++}{
%			start = i * lenSplit\;
%			end = (i+1) * lenSplit\;
%			arrSplitted.append[start:end]\;
%		}
%		\BlankLine
%		
%		\Return arrSplitted;
%	}
%	
%	\caption{\textit{Pseudocode} untuk memecah \textit{data} menjadi 10 bagian}	
%	\label{code:split}
%\end{kode}
%
%Setelah masing-masing fitur dipecah menjadi 10 bagian, \saya~melakukan penggabungan antar fitur sebagai \textit{input} untuk melakukan eksperimen. Seperti yang dijelaskan pada tahap sebelumnya, \saya~menggunakan dua arsitektur RNNs. Hasil dari eksperimen tersebut ditulis dalam sebuah berkas dengan format JSON yang nantinya akan menjadi \textit{input} pada tahap evaluasi. Berikut merupakan implementasi eksperimen dengan masing-masing arsitektur tersebut.
%
%\begin{kode}
%	
%	\SetKwInOut{Input}{Input}
%	\SetKwInOut{Output}{Output}
%	
%	\SetKwProg{Fn}{Function}{ is}{end}
%	arrAllFeature = []\;
%	\ForEach{feature in arrSplitted}{
%		arrAllFeature.join(feature)\;
%	}
%	\BlankLine
%	
%	\For{i=0; i<10;i++}{
%		training = arrSplitted[0:i] + arrSplitted[i+1:10]
%		testing = arrSplitted[i]\;
%		
%		\BlankLine
%		result1 = lstm1(training, testing)\;
%		result2 = lstm2(training, testing)\;
%		
%		\BlankLine
%		writeToJSON(result1)\;
%		writeToJSON(result2)\;
%	}
%	\caption{\textit{Pseudocode} untuk melakukan eksperimen}
%	\label{code:eksperimen}	
%\end{kode}
%
%\section{Evaluasi}
%Dalam melakukan implementasi pada tahap evaluasi, \saya~menghitung nilai \textit{prescision, recall} dan \textit{F-Measure} untuk mengukur tingkat keakuratan model yang dikembangkan pada tahap sebelumnya. \Saya~menggunakan aturan yang telah dijelaskan pada Bab 3. Berikut merupakan implementasi kode untuk melakukan evaluasi.
%
%\begin{kode}
%	
%	\SetKwInOut{Input}{Input}
%	\SetKwInOut{Output}{Output}
%	
%	\SetKwProg{Fn}{Function}{ is}{end}
%	resultTag = load(resultRNN)\;
%	originalTag = load(originalTag)\;
%	\BlankLine
%	
%	TP = newHash()\;
%	FP = newHash()\;
%	FN = newHash()\;
%	\For{i = 0; i < len(resultTag); i++}{
%		sentenceResult = resultTag[i]\;
%		sentenceOriginal = originalTag[i]\;
%		\For{j = 0; j < len(sentenceOriginal); i++}{
%			wordResult = sentenceResult[j]\;
%			wordOri = sentenceOriginal[j]\;
%			\uIf{wordOri != O}{
%				\uIf{wordResult != O}{
%					\uIf{wordOri == wordResult}{
%						TP[wordOri] += 1\;
%					}
%					\Else{
%						FN[wordOri] += 1\;
%					}
%				}
%				\Else{
%					FN[wordOri] += 1\;
%				}
%			}
%			\Else{
%				\uIf{wordResult != O}{
%					FP[wordOri] += 1\;
%				}
%			}
%		}
%	}
%	\BlankLine
%
%	prec = newHash()\;
%	rec = newHash()\;
%	fMeas = newHash()\;
%	\ForEach{label in TP}{
%		prec[label] = TP[label] / (TP[label] + FP[label])\;
%		rec[label] = TP[label] / (TP[label] + FN[label])\;
%		fMeas[label] = 2 * 	(prec[label] * rec[label]) / (prec[label] + rec[label])\;
%	}
%	\BlankLine
%	
%	\ForEach{label in prec}{
%		print "Precission", label, prec[label]\;
%		print "Recall", label, rec[label]\;
%		print "F-Measure", label, fmeas[label]\;
%	}
%	\BlankLine
%	\caption{\textit{Pseudocode} untuk melakukan evaluasi}	
%	\label{code:evaluasi}
%\end{kode}