%!TEX root = skripsi.tex
%-----------------------------------------------------------------------------%
\chapter{\babEmpat} \label{eksperimen}
%-----------------------------------------------------------------------------%

This chapter explans the implementations of the methodology explained in chapter 3. It includes the implementation of data annotation and pre-processing, model development, experiment, as well as the evaluation.

\section{Computer Specification}
For every experiment, we use GPU-based virtual server provided by Kata.ai. The specifications of the server are explained as follows.

\begin{table}
	\centering
	\caption{Server Specifications}
	\begin{tabular}{|l|l|}
		\hline
		\textbf{Processor} & i7-4770S \\ \hline
		\textbf{Number of Cores} & 8 core \\ \hline
		\textbf{Processor Frequency} & 3.1 GHz per core \\ \hline
		\textbf{RAM} & 8 GB \\ \hline
		\textbf{Operating System} & Ubuntu 14 \\ \hline
	\end{tabular}
	\label{table:spesifikasi hardware}
\end{table}

The server uses 8-core i7 processor with 3.1 GHz per core frequency. The size of the RAM is 8 GB. We use Ubuntu 14 distribution as the operating system.

\section{Data Annotation and Pre-processing}
For data annotation, we use an in-house tool provided by Kata.ai, named \textit{kata-annotator}. After finish annotating, the tool outputs the tokenized annotation result as JSON in BIO format. An example is given as follows:

[\\
	{\\
		"data": ["Aku", "pengen", "makan", "ayam", "goreng", "dong"],\\
		"label": ["B-AGENT", "B-MD", "B-PRED", "B-PATIENT", "I-PATIENT", "O"]\\
	},\\
	{\\
		"data": ["Kamu", "gak", "tidur", ",", "Andi", "?"],\\
		"label": ["B-AGENT", "O", "B-PRED", "O", "B-GREET", "O"]\\
	}\\
]

The total amount of data to be annotated was 9000 sentences. The data was annotated by three linguists with each of them annotating different set of data containing 3000 sentences for 8 weeks. In order to align the annotation understanding between them, the three linguists annotated the same trial set consisting of 100 sentences before starting to annotate the real one. The annotation differences found are then discussed in order to align the understanding between them.

After 8 weeks of annotation, the total amount of data that has been annotated is 8000 sentences. The other 1000 sentences missing is due to one annotator that could not complete the annotation on time. It turns out there are only 5000 sentences which contain predicate in it. These 5000 sentences are the main data set to be trained, validated, and tested. This data set is split into training, validation, and testing sets with the ratio 60:20:20. 

\section{Model Development}
\subsection{Feature Extraction}
\subsubsection{Word Embedding}
% Jelasin pake gensim's word2vec. Parameter nya apa aja. Data training yang digunakan darimana.

\begin{kode}
	
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\SetKwProg{Fn}{Function}{ is}{end}
	\Fn{trainWordEmbeddingModel(corpus, contextWindow, vectorDimension)}{
		\Input{training corpus, context window, vector dimension}
		\Output{Word2Vec model}
		
		\BlankLine
		model = Word2Vec.createModel(corpus, contextWindow, vectorDimension)
		
		\BlankLine
		\Return model;
	}
	
	\caption{A pseudocode to train word embedding model using Word2Vec}
	\label{code:trainword2ve}
\end{kode}

% jelasin kode train word embedding

\begin{kode}
	
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\SetKwProg{Fn}{Function}{ is}{end}
	\Fn{wordToVector(model, arrWord)}{
		\Input{word embedding model, array of words of a sentence}
		\Output{array of word vectors}
		
		\BlankLine
		arrVector = []\;
		\ForEach{word in arrWord}{
			arrVector.append(model.getVector(word))
		}
		
		\BlankLine
		\Return arrVector;
	}
	
	\caption{A pseudocode to transform words into vectors by word embedding model}
	\label{code:ekstraksiownword}
\end{kode}
% jelasin kode convert word embedding

\subsubsection{POS Tag}
For POS Tag feature, we use the gold-standard POS tag annotated by the three linguists. The annotation tool \textit{kata-annotator} is also used for annotating the POS tag. The output example of the POS tag from the tool is given as follows:

[\\
{\\
	"data": ["Aku", "pengen", "makan", "ayam", "goreng", "dong"],\\
	"label": ["NN", "ADV", "V", "NN", "V", "INTJ"]\\
},\\
{\\
	"data": ["Kamu", "gak", "tidur", ",", "Andi", "?"],\\
	"label": ["NN", "NEG", "V", "O", "NN", "O"]\\
}\\
]

% jelasin sedikit tentang contoh output tools nya

\begin{kode}
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	
	\SetKwProg{Fn}{Function}{ is}{end}
	\Fn{convertPOSTagToOneHotVector(arrPOS)}{
		\Input{array of POS tags of a sentence}
		\Output{array of one hot vector}
		\BlankLine
		
		posTagFeature = []\;
		\ForEach{pos in arrPOS}{
			posTagFeature.append(pos.convertToOneHotVector())
		}
		\BlankLine
		
		\Return posTagFeature;
	}
	
	\caption{A pseudocode for converting POS tags of a sentence into one hot vectors}
	\label{code:ekstraksipostag}
	
\end{kode}

% jelasin sedikit tentang convert POS tag into one hot vector

\subsubsection{Neighboring Word Embeddings}
% jelasin ada dua, yaitu extract 1 word vector before and after. Pada sebelum token pertama dan setelah token terakhir ditambah padding vektor 0.

\begin{kode}	
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	
	\SetAlgoLined
	\SetKwProg{Fn}{Function}{ is}{end}
	\Fn{oneWordBeforeExtract(sentenceVector)}{
		\Input{array of vector in a sentence}
		\Output{array of vector}
		\BlankLine
		
		oneWordBeforeFeature = []\;
		oneWordBeforeFeature.append(zeroth)\;
		\For{i in 1...sentenceVector.length}{
			oneWordBeforeFeature.append(sentenceVector[i-1])\;
		}
		\BlankLine
		
		\Return oneWordBeforeFeature;
		
	}
	
	\BlankLine
	
	\caption{\textit{Pseudocode} untuk melakukan ekstraksi fitur 1 kata sebelum}
	\label{code:ekstraksi1wordbef}	
\end{kode}

\begin{kode}
	
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	
	\SetAlgoLined
	\SetKwProg{Fn}{Function}{ is}{end}
	\Fn{oneWordBeforeExtract(sentenceVector)}{
		\Input{array of vector in a sentence}
		\Output{array of vector}
		\BlankLine
		
		oneWordAfterFeature = []\;
		\For{i in 0...sentenceVector.length - 1}{
			oneWordAfterFeature.append(sentenceVector[i+1])\;
		}
		oneWordAfterFeature.append(zeroth)\;
		\BlankLine
		
		\Return oneWordAfterFeature;
		
	}
	
	\BlankLine	
	\caption{\textit{Pseudocode} untuk melakukan ekstraksi fitur 1 kata sesudah}
	\label{code:ekstraksi1wordaft}
\end{kode}

\subsection{Model Architecture}
% Jelasin gambaran umum apa aja architecture nya.

\subsubsection{Vanilla LSTM (LSTM)}
\subsubsection{Bi-Directional LSTM (BLSTM)}
\subsubsection{CNN-BLSTM}
\subsubsection{Context-Aware BLSTM (CA-BLSTM)}

\section{Experiment}

\section{Evaluation}

% PUNYA WAHID
\section{Pengembangan Model}
\subsection{Ekstraksi Fitur}
Ekstraksi fitur dilakukan dengan menggunakan program yang diimplementasikan dalam bahasa Python. Keluaran dari ekstraksi fitur ini adalah vektor kata untuk masing-masing kalimat yang disimpan dalam format JSON. Masing-masing kalimat dalam sebuah \textit{post} disimpan dalam sebuah \textit{array} yang kemudian keseluruhan \textit{post} disimpan dalam \textit{hash} dengan indeks yang telah didefinisikan pada saat tahap pengumpulan data.

\subsubsection{Fitur Kata Itu Sendiri}

\subsection{Pengusulan Arsitektur RNNs}
Sesuai dengan yang telah dijelaskan pada Bab 3, \saya~mengusulkan dua arsitektur RNNs yang akan digunakan pada tahap eksperimen. Pada bagian ini \saya~akan menjelaskan implementasi dari masing-masing arsitektur tersebut. Dalam melakukan implementasi RNNs, \saya~menggunakan \textit{library} Keras \citep{chollet2015keras} dalam bahasa Python. Keras sendiri dapat berjalan di atas dua \textit{library deep learning} lain, yaitu Theano dan Tensorflow, namun dalam penelitian ini \saya~menggunakan Theano. \Saya~menggunakan \textit{Sequential model} yang merupakan layer \textit{linear stack} dalam mengembangkan model dan jenis RNNs yang \saya~gunakan dalam penelitian ini adalah LSTMs. Terkait dengan jumlah \textit{timesteps}, LSTMs membutuhkan jumlah yang tedefinisi dari awal. Oleh karena itu, jumlah \textit{timestep} merupakan jumlah kata dari kalimat terpanjang di dalam korpus ($ l $). Apabila terdapat kalimat yang panjangnya kurang dari $ l $, vektor dari kalimat tersebut akan di-\textit{padding} atau ditambahkan vektor $ \vec{0} $ sampai panjangnya $ l $. Dalam melakukan \textit{padding}, \saya~menggunakan fungsi \textit{padsequences} yang sudah terdapat di dalam Keras.

\subsubsection{LSTMs 1 layer}
LSTMs 1 layer yang dimaksud adalah model yang digunakan memiliki satu layer LSTMs saja dan semua fitur yang menjadi input program digabung terlebih dahulu menjadi satu buah \textit{array}.

\begin{kode}
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	
	\SetKwProg{Fn}{Function}{ is}{end}
	\Fn{lstm1(arrTraining, arrTesting)}{
		\Input{training data, testing data}
		\Output{predicted label}
		\BlankLine
		
		doPadSequences(arrTraining)\;
		doPadSequences(arrTesting)\;
		shape = arrTraning.shape()\;
		\BlankLine
		
		model = Sequential()\;
		model.add(Masking(input\char`_shape:shape))]\;
		model.add(LSTM(output = shape/2))\;
		model.add(TimeDistributed(Dense(output = 9)))\;
		\BlankLine
		
		model.input(arrTraining)\;
		prediction = model.predict(arrTesting)\;
		\BlankLine
		
		\Return prediction;
	}
	
	\caption{\textit{Pseudocode} untuk arsitektur LSTMs 1 layer}
	\label{code:lstm1}
\end{kode}

Seperti yang telah dijelaskan pada Bab 3, susunan layer yang \saya~gunakan terdiri dari \textit{Masking Layer}, LSTMs Layer, dan \textit{Time Distributed Layer} yang masing-masing \textit{timestep} berisi \textit{Dense Layer}. Untuk \textit{Masking Layer}, dimensi yang menjadi parameter tergantung dari \textit{array} yang menjadi masukan, untuk LSTMs Layer, dimensi masukan sama dengan dimensi \textit{Masking Layer} dan dimensi keluaran untuk masing-masing \textit{timesteps} adalah panjang input dalam satu \textit{timestep} dibagi 2. Untuk masing-masing Dense Layer, dimensi masukan yang diminta sama dengan dimensi keluaran pada LSTMs Layer dan dimensi keluaran sesuai dengan jumlah kelas yang telah didefinisikan.

Masukan yang diminta yaitu \textit{array} yang masing-masing elemennya merupakan \textit{array} dari vektor fitur dan sudah digabung menjadi satu. Keluaran yang diminta merupakan hasil dari pelabelan otomatis dari program ini. Kode \ref{code:lstm1} merupakan kode untuk mengimplementasikan model ini.

\subsubsection{LSTMs 2 Layer Multi-Input}
LSTMs 2 layer yang dimaksud yaitu terdapat dua layer, layer pertama untuk menerima \textit{input} yang setiap kelompok fitur menjadi \textit{input} bagi LSTMs masing-masing. Misalnya terdapat 3 kelompok fitur, masing-masing kelompok tadi akan menjadi input bagi layer LSTMs masing-masing. Layer kedua sebagai penggabung hasil dari tingkat pertama.

\begin{kode}
	
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	
	\SetKwProg{Fn}{Function}{ is}{end}
	\Fn{lstm2(groupOfArrTraining, groupOfArrTraining)}{
		\Input{grop of training data, group of testing data}
		\Output{predicted label}
		\BlankLine
		
		doPadSequences(groupOfArrTraining)\;
		doPadSequences(groupOfArrTraining)\;
		\BlankLine
		
		modelArr = []\;
		\ForEach{groupFeature in groupOfArrTraining}{
			shape = arrTraning.shape()\;
			model = Sequential()\;
			model.add(Masking(input\char`_shape:shape))]\;
			model.add(LSTM(output = shape/2))\;
			modelArr.append(model)\;
		}
		\BlankLine
		
		mainModel = Sequential()\;
		mainModel.add(Merge(mode='concat', modelArr))]\;
		mainModel.add(LSTM(output = 32))\;
		mainModel.add(TimeDistributed(Dense(output = 9)))\;
		\BlankLine
		
		mainModel.input(groupOfArrTraining)\;
		prediction = mainModel.predict(groupOfArrTraining)\;
		\BlankLine
		
		\Return prediction;
	}
	
	\caption{\textit{Pseudocode} untuk arsitektur LSTMs 2 layer multi-\textit{input}}
	\label{code:lstm2}	
\end{kode}

Layer pada tingkat pertama terdiri dari \textit{Masking Layer} dan sebuah Layer LSTMs. Untuk dimensi \textit{input} dan \textit{output} \textit{Masking Layer} secara otomatis mengikuti dimensi dari data masukan. Dimensi \textit{output} dari Layer LSTMs yaitu dimensi awal dibagi 2. Pada layer tingkat kedua, layer tersebut terdiri dari \textit{Merge Layer}, \textit{Time Distributed} dengan masing-masing \textit{timestep} merupakan \textit{Dense Layer} dan sebuah Layer LSTMs. Keluaran dari \textit{Merge Layer} sesuai dengan total dimensi \textit{output} dari masing-masing LSTMs di tingkat 1. Dimensi keluaran dari masing-masing \textit{Dense Layer} yaitu sesuai jumlah kelas.
Masukan yang diminta yaitu \textit{array} yang masing-masing elemennya merupakan \textit{array} dari vektor fitur dan sudah digabung menjadi satu. Keluaran yang diminta merupakan hasil dari pelabelan otomatis dari program ini. Kode \ref{code:lstm2} merupakan kode untuk mengimplementasikan model ini.

\section{Eksperimen}
Pada tahap ini \saya~melakukan eksperimen model yang dikembangkan pada tahap sebelumnya. Sebelum masuk ke tahap eksperimen, \saya~melakukan beberapa tahap pra-eksperimen seperti melakukan pemecahan data sebagai implementasi \textit{cross-fold validation}. \Saya~memecah data menjadi 10 bagian dan disimpan dalam sebuah \textit{array} untuk masing-masing fitur. Berikut merupakan \textit{pseudocode} untuk melakukan pemecahan data

\begin{kode}

	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	
	\SetKwProg{Fn}{Function}{ is}{end}
	\Fn{splitting(featureArr)}{
		\Input{array of feature}
		\Output{splitted array of feature}
		\BlankLine
		
		lenSplit = len(featureArr)/10\;
		arrSplitted = []\;
		\For{i=0; i<10;i++}{
			start = i * lenSplit\;
			end = (i+1) * lenSplit\;
			arrSplitted.append[start:end]\;
		}
		\BlankLine
		
		\Return arrSplitted;
	}
	
	\caption{\textit{Pseudocode} untuk memecah \textit{data} menjadi 10 bagian}	
	\label{code:split}
\end{kode}

Setelah masing-masing fitur dipecah menjadi 10 bagian, \saya~melakukan penggabungan antar fitur sebagai \textit{input} untuk melakukan eksperimen. Seperti yang dijelaskan pada tahap sebelumnya, \saya~menggunakan dua arsitektur RNNs. Hasil dari eksperimen tersebut ditulis dalam sebuah berkas dengan format JSON yang nantinya akan menjadi \textit{input} pada tahap evaluasi. Berikut merupakan implementasi eksperimen dengan masing-masing arsitektur tersebut.

\begin{kode}
	
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	
	\SetKwProg{Fn}{Function}{ is}{end}
	arrAllFeature = []\;
	\ForEach{feature in arrSplitted}{
		arrAllFeature.join(feature)\;
	}
	\BlankLine
	
	\For{i=0; i<10;i++}{
		training = arrSplitted[0:i] + arrSplitted[i+1:10]
		testing = arrSplitted[i]\;
		
		\BlankLine
		result1 = lstm1(training, testing)\;
		result2 = lstm2(training, testing)\;
		
		\BlankLine
		writeToJSON(result1)\;
		writeToJSON(result2)\;
	}
	\caption{\textit{Pseudocode} untuk melakukan eksperimen}
	\label{code:eksperimen}	
\end{kode}

\section{Evaluasi}
Dalam melakukan implementasi pada tahap evaluasi, \saya~menghitung nilai \textit{prescision, recall} dan \textit{F-Measure} untuk mengukur tingkat keakuratan model yang dikembangkan pada tahap sebelumnya. \Saya~menggunakan aturan yang telah dijelaskan pada Bab 3. Berikut merupakan implementasi kode untuk melakukan evaluasi.

\begin{kode}
	
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	
	\SetKwProg{Fn}{Function}{ is}{end}
	resultTag = load(resultRNN)\;
	originalTag = load(originalTag)\;
	\BlankLine
	
	TP = newHash()\;
	FP = newHash()\;
	FN = newHash()\;
	\For{i = 0; i < len(resultTag); i++}{
		sentenceResult = resultTag[i]\;
		sentenceOriginal = originalTag[i]\;
		\For{j = 0; j < len(sentenceOriginal); i++}{
			wordResult = sentenceResult[j]\;
			wordOri = sentenceOriginal[j]\;
			\uIf{wordOri != O}{
				\uIf{wordResult != O}{
					\uIf{wordOri == wordResult}{
						TP[wordOri] += 1\;
					}
					\Else{
						FN[wordOri] += 1\;
					}
				}
				\Else{
					FN[wordOri] += 1\;
				}
			}
			\Else{
				\uIf{wordResult != O}{
					FP[wordOri] += 1\;
				}
			}
		}
	}
	\BlankLine

	prec = newHash()\;
	rec = newHash()\;
	fMeas = newHash()\;
	\ForEach{label in TP}{
		prec[label] = TP[label] / (TP[label] + FP[label])\;
		rec[label] = TP[label] / (TP[label] + FN[label])\;
		fMeas[label] = 2 * 	(prec[label] * rec[label]) / (prec[label] + rec[label])\;
	}
	\BlankLine
	
	\ForEach{label in prec}{
		print "Precission", label, prec[label]\;
		print "Recall", label, rec[label]\;
		print "F-Measure", label, fmeas[label]\;
	}
	\BlankLine
	\caption{\textit{Pseudocode} untuk melakukan evaluasi}	
	\label{code:evaluasi}
\end{kode}