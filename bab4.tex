%!TEX root = skripsi.tex
%-----------------------------------------------------------------------------%
\chapter{\babEmpat} \label{eksperimen}
%-----------------------------------------------------------------------------%

This chapter explains the implementations of the methodology explained in chapter 3. It includes the implementation of data annotation and pre-processing, feature extraction, and model architecture.

\section{Computer Specification}
Every experiment uses GPU-based virtual server provided by Kata.ai. The specifications of the server are explained as follows.

\begin{table}
	\centering
	\caption{Server Specifications}
	\begin{tabular}{|l|l|}
		\hline
		\textbf{GPU} & Nvidia Tesla M60 \\ \hline
		\textbf{Number of Cores} & 6 \\ \hline
		\textbf{RAM} & 56.00 GiB \\ \hline
		\textbf{Operating System} & Ubuntu 16.04.2 \\ \hline
	\end{tabular}
	\label{table:spesifikasi hardware}
\end{table}

The server uses GPU Nvidia Tesla M60 with 6 cores. The size of the RAM is 56.00 GiB. The operating system used is Ubuntu 16.04.2.

\section{Data Annotation and Pre-processing}
For data annotation, we use an in-house tool provided by Kata.ai, named \textit{kata-annotator}. The total amount of data to be annotated was 9000 sentences. The data was annotated by three linguists with each of them annotating different set of data containing 3000 sentences for 8 weeks. In order to align the annotation understanding between them, the three linguists annotated the same trial set consisting of 100 sentences before starting to annotate the real one. The annotation differences found are then discussed in order to align the understanding between them.

After 8 weeks of annotation, the total amount of data that has been annotated is 8000 sentences. The other 1000 sentences missing is due to one annotator that could not complete the annotation on time. After finish annotating, the tool outputs the tokenized annotation result as JSON in BIO format. An example is given as follows:

\fbox{%
	\parbox{1.0\linewidth}{%
		[\{\\
				"data": ["Aku", "pengen", "makan", "ayam", "goreng", "dong"],\\
				"label": ["B-AGENT", "B-MD", "B-PRED", "B-PATIENT", "I-PATIENT", "O"]\\
			\},\\
			\{\\
				"data": ["Kamu", "gak", "tidur", ",", "Andi", "?"],\\
				"label": ["B-AGENT", "O", "B-PRED", "O", "B-GREET", "O"]\\
			\}]
	}%
}
\\

It turns out that there are only 5036 sentences which contain predicate in it. These 5036 sentences are the main data set to be trained and tested.

\begin{kode}
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	
	\SetKwProg{Fn}{Function}{ is}{end}
	\Fn{convertLabelToOneHotVector(arrLabel)}{
		\Input{array of labels of a sentence}
		\Output{array of one hot vectors}
		\BlankLine
		
		oneHotVectorLabel = []\;
		\ForEach{label in arrLabel}{
			oneHotVectorLabel.append(label.convertToOneHotVector())
		}
		\BlankLine
		
		\Return oneHotVectorLabel;
	}
	
	\caption{A pseudocode for converting labels of a sentence into one-hot-vectors}
	\label{code:labeltoone}
\end{kode}

The label is then converted into a one-hot-vector representation which is presented in Pseudocode~\ref{code:labeltoone}. The function \textit{convertLabelToOneHotVector} takes an array of labels from one sentence as an input. Each label in the array is then converted into its one-hot-vector representation. All the converted results are appended into a new array called \textit{oneHotVectorLabel}. The function finally returns the array of one-hot-vectors.

\section{Feature Extraction}
There are three features to be extracted including Word Embedding, POS tag, and Neighboring Word Embeddings. This section explains the implementation of each feature extraction. All codes are implemented using Python.

\subsection{Word Embedding}
% Jelasin pake gensim's word2vec. Parameter nya apa aja. Data training yang digunakan darimana.
We use Gensim's Word2Vec~\citep{gensim} as the library for training the word embedding model as well as converting words into vectors. Pseudocode~\ref{code:trainword2ve} explains on how to train the word embedding model.
\begin{kode}
	
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\SetKwProg{Fn}{Function}{ is}{end}
	\Fn{trainWordEmbeddingModel(corpus, contextWindow, vectorDimension)}{
		\Input{training corpus, context window, vector dimension}
		\Output{Word2Vec model}
		
		\BlankLine
		model = Word2Vec.createModel(corpus, contextWindow, vectorDimension)
		
		\BlankLine
		\Return model;
	}
	
	\caption{A pseudocode to train word embedding model using Word2Vec}
	\label{code:trainword2ve}
\end{kode}

There are two parameters, which are context window and vector dimension. Context window determines the area of interest in building the word embedding model. Vector dimension represents the length of the output vector. In this work, the context window and vector dimension used are 5 and 64, respectively. The output is the trained word embedding model.

\begin{kode}
	
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\SetKwProg{Fn}{Function}{ is}{end}
	\Fn{wordToVector(model, arrWord)}{
		\Input{trained word embedding model, array of words of a sentence}
		\Output{array of word vectors}
		
		\BlankLine
		arrVector = []\;
		\ForEach{word in arrWord}{
			arrVector.append(model.getVector(word))
		}
		
		\BlankLine
		\Return arrVector;
	}
	
	\caption{A pseudocode to transform words into vectors by word embedding model}
	\label{code:ekstraksiownword}
\end{kode}
% jelasin kode convert word embedding
% jelasin kode train word embedding
Pseudocode~\ref{code:ekstraksiownword} shows on how to use the trained word embedding model to convert words into vectors. The function initially takes the trained model and an array of words from a sentence. Each word is then converted into the vector using the trained model. The converted words are appended in a new array called \textit{arrVector}. The function finally returns this new array.

\subsection{POS Tag}
For POS Tag feature, we use the gold-standard POS tag annotated by the three linguists. The annotation tool \textit{kata-annotator} is also used for annotating the POS tag. The output example of the POS tag from the tool in a form of JSON is given as follows:

\fbox{%
	\parbox{1.0\linewidth}{%
		[\\
		\{\\
		"data": ["Aku", "pengen", "makan", "ayam", "goreng", "dong"],\\
		"label": ["NN", "ADV", "V", "NN", "V", "INTJ"]\\
		\},\\
		\{\\
		"data": ["Kamu", "gak", "tidur", ",", "Andi", "?"],\\
		"label": ["NN", "NEG", "V", "O", "NN", "O"]\\
		\}\\
		]
	}%
}
\\

% jelasin sedikit tentang contoh output tools nya
The JSON file consists of an array of sentences. Each word in a sentence is labeled with the POS tag accordingly.

\begin{kode}
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	
	\SetKwProg{Fn}{Function}{ is}{end}
	\Fn{convertPOSToOneHotVector(arrPOS)}{
		\Input{array of POS tags of a sentence}
		\Output{array of one hot vector}
		\BlankLine
		
		posTagFeature = []\;
		\ForEach{pos in arrPOS}{
			posTagFeature.append(pos.convertToOneHotVector())
		}
		\BlankLine
		
		\Return posTagFeature;
	}
	
	\caption{A pseudocode for converting POS tags of a sentence into one hot vectors}
	\label{code:ekstraksipostag}
\end{kode}

% jelasin sedikit tentang convert POS tag into one hot vector
Each of the POS tag in a sentence is then converted into one-hot-vector. The implementation is presented in Pseudocode~\ref{code:ekstraksipostag}. The function \textit{convertPOSToOneHotVector} initially takes array of POS tags from a sentence as the input. Each POS tag is converted into its respective one-hot-vector. The results are appended into a new array called \textit{posTagFeature}. The function finally returns this new array as the one-hot-vector representations of POS tags in a sentence.

\subsection{Neighboring Word Embeddings}
% jelasin ada dua, yaitu extract 1 word vector before and after. Pada sebelum token pertama dan setelah token terakhir ditambah padding vektor 0.
For neighboring word embeddings, we extract one vector on the left and one on the right of the word being processed. Pseudocode~\ref{code:neighboringwe} shows the implementation of extracting neighboring word embeddings.

\begin{kode}	
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	
	\SetAlgoLined
	\SetKwProg{Fn}{Function}{ is}{end}
	\Fn{extractNeighboringWordEmbedding(sentenceVector)}{
		\Input{array of word embedding vectors of a sentence}
		\Output{array of neighboring word embedding vectors}
		\BlankLine
		
		window = 1\;
		vectorDimension = getVectorDimension(sentenceVector)\;
		padded = window * [vectorDimension*[0.]] + sequence + window * [vectorDimension*[0.]]\;
		\BlankLine
		
		neighboringVectors = []\;
		\For{i in 0...sentenceVector.length - 1}{
			left = [item for sublist in padded[i:(i + window)] for item in sublist]\;
			right = [item for sublist in padded[(i+ window + 1):(i + (2*window + 1))] for item in sublist]\;
			concate = left + right\;
			neighboringVectors.append(concate)\;
		}
		\BlankLine
		
		\Return neighboringVectors;
		
	}	
	\caption{A pseudocode to extract neighboring word embeddings}
	\label{code:neighboringwe}	
\end{kode}

In this function, the parameter that can be set is the \textit{window}. Since we only extract one word on the left and another on the right of the word being processed, the value of parameter \textit{window} is 1.

\section{Model Architecture}
As explained in chapter 3, the model architecture consists of main layer and additional layer. The main layer choices include vanilla LSTM (LSTM), Bi-Directional LSTM (BLSTM), Deep BLSTM (BLSTM), DBLSTM-Zhou, and DBLSTM-Highway. On the other hand, the additional layer options include Convolutional Neural Networks (CNN) and attention mechanism. 

We use Keras 2.0~\citep{chollet2015} as our deep learning library with Tensorflow 1.1.0 backend for all the architectures. For constructing the model, we use the Functional API of Keras. The model in Keras only receives an input data with a fixed number of time steps for all sentences. Suppose that the maximum number of time steps in our data is $l$. Thus, all sentences in our data whose number of time steps is lower than $l$ have to be padded with vector $ \vec{0}$ in order to get a fixed number of time steps of $l$. To do the padding, we use \textit{padsequences} function available from Keras. All codes are implemented using Python.

\subsection{Main Layers}
The main layer choices include vanilla LSTM (LSTM), Bi-Directional LSTM (BLSTM), Deep BLSTM (BLSTM), DBLSTM-Zhou, and DBLSTM-Highway. This section explains the implementation of each architecture.

\subsubsection{Vanilla LSTM (LSTM)}
Vanilla LSTM (LSTM) consists of only one layer of forward LSTM. Pseudocode~\ref{code:olstm} shows the implementation of the LSTM architecture.

\begin{kode}
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}

	\SetKwProg{Fn}{Function}{ is}{end}
	\Fn{lstm(xTrain, yTrain, timesteps, features, xTest, yTest)}{
		\Input{x train, y train, number of time steps, number of features, x test, y test}
		\Output{trained model, testing prediction result}
		\BlankLine
		
		input = Input(shape=(timesteps, features))\;
		forward = LSTM(units=128, returnSequences=True, recurrentDropout=0.2)(input)\;
		dropout = Dropout(0.2)(forward)\;
		output = TimeDistributed(Dense(units=17, activation='softmax'))(dropout)\;
		model = Model(inputs=[input], outputs=[output])\;
		\BlankLine
		
		model.compile(loss='categoricalCrossentropy', optimizer='adam')\;
		\BlankLine
		
		model.fit(xTrain, yTrain, epochs=50, batchSize=150)\;		
		prediction = model.predict(xTest)\;
		\BlankLine
		
		\Return model, prediction\;
	}
	
	\caption{A pseudocode for building and training vanilla LSTM architecture}
	\label{code:olstm}
\end{kode}

The Pseudocode~\ref{code:olstm} takes x train, y train, number of time steps, number of features, x test and y test as the inputs. The model starts with the defining the input layer, with the input shape of (timesteps, features). The input layer is then connected to the LSTM layer thas has 128 hidden units. These hidden units are the output of the LSTM layer. We use recurrent dropout in LSTM, as recommended by~\cite{he2017deep}. The recurrent dropout used is 0.2. We also use dropout layer on top of the LSTM layer by the value of 0.2. The output of the dropout layer is connected to the time-distributed dense layer with softmax activation function. These last layer produces the labels of semantic roles. The model is trained with categorical crossentropy loss function and Adam optimizer. The number of epochs and batch size used are both 50. After the model has been trained, we use it to predict the semantic roles the x test data which later will be evaluated. The function returns the trained model as well as the prediction result of the test data.

\subsubsection{Bi-Directional LSTM (BLSTM)}
Bi-Directional LSTM (BLSTM) consists of two layers of LSTM. The first layer is moving forward and the other is moving backward. Pseudocode~\ref{code:blstm} shows the implementation of the BLSTM architecture.

\begin{kode}
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	
	\SetKwProg{Fn}{Function}{ is}{end}
	\Fn{blstm(xTrain, yTrain, timesteps, features, xTest, yTest)}{
		\Input{x train, y train, number of time steps, number of features, x test, y test}
		\Output{trained model, testing prediction result}
		\BlankLine
		
		input = Input(shape=(timesteps, features))\;
		\BlankLine
		
		forward = LSTM(units=128, returnSequences=True, recurrentDropout=0.2)(input)\;
		backward = LSTM(units=128, returnSequences=True, goBackwards=True, recurrentDropout=0.2)(input)\;
		merged = Concatenate([forward, backward])\;
		dropout = Dropout(0.2)(merged)\;
		\BlankLine
		
		output = TimeDistributed(Dense(units=17, activation='softmax'))(dropout)\;
		\BlankLine
		
		model = Model(inputs=[input], outputs=[output])\;
		model.compile(loss='categoricalCrossentropy', optimizer='adam')\;
		model.fit(xTrain, yTrain, epochs=50, batchSize=150)\;		
		prediction = model.predict(xTest)\;
		\BlankLine
		
		\Return model, prediction\;
	}
	
	\caption{A pseudocode for building and training BLSTM architecture}
	\label{code:blstm}
\end{kode}

The Pseudocode~\ref{code:blstm} takes x train, y train, number of time steps, number of features, x test and y test as the inputs. The model starts with the defining the input layer, with the input shape of (timesteps, features). The input layer is then connected to two LSTM layers: forward layer and backward layer. Both forward and backward LSTM layers have 128 hidden units. The recurrent dropout used is 0.2. The output of both forward and backward layers are concatenated, resulting a vector whose length is 256. We also use dropout layer on top of the concatenated layer by the value of 0.2. The output of the dropout layer is connected to the time-distributed dense layer with softmax activation function. This last layer produces the labels of semantic roles. The model is trained with categorical crossentropy loss function and Adam optimizer. The number of epochs and batch size used are 50 and 150, respectively. After the model has been trained, we use it to predict the semantic roles the x test data which later will be evaluated. The function returns the trained model as well as the prediction result of the test data.

\subsubsection{Deep BLSTM (DBLSTM)}
Deep BLSTM (BLSTM) consists of two pairs of BLSTM. Pseudocode~\ref{code:dblstm} shows the implementation of the BLSTM architecture.

\begin{kode}
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	
	\SetKwProg{Fn}{Function}{ is}{end}
	\Fn{blstm(xTrain, yTrain, timesteps, features, xTest, yTest)}{
		\Input{x train, y train, number of time steps, number of features, x test, y test}
		\Output{trained model, testing prediction result}
		\BlankLine
		
		input = Input(shape=(timesteps, features))\;
		\BlankLine
		
		forward1 = LSTM(units=128, returnSequences=True, recurrentDropout=0.2)(input)\;
		backward1 = LSTM(units=128, returnSequences=True, goBackwards=True, recurrentDropout=0.2)(input)\;
		merged1 = Concatenate([forward, backward])\;
		dropout1 = Dropout(0.2)(merged)\;
		\BlankLine
		
		forward2 = LSTM(units=128, returnSequences=True, recurrentDropout=0.2)(dropout1)\;
		backward2 = LSTM(units=128, returnSequences=True, goBackwards=True, recurrentDropout=0.2)(dropout1)\;
		merged2 = Concatenate([forward2, backward2])\;
		dropout2 = Dropout(0.2)(merged2)\;
		\BlankLine
		
		output = TimeDistributed(Dense(units=17, activation='softmax'))(dropout2)\;
		\BlankLine
		
		model = Model(inputs=[input], outputs=[output])\;
		model.compile(loss='categoricalCrossentropy', optimizer='adam')\;
		model.fit(xTrain, yTrain, epochs=50, batchSize=150)\;		
		prediction = model.predict(xTest)\;
		\BlankLine
		
		\Return model, prediction\;
	}
	
	\caption{A pseudocode for building and training DBLSTM architecture}
	\label{code:dblstm}
\end{kode}

The input layer is first connected to two LSTM layers, the first one for going forward and another for going backward. Both forward and backward LSTM layers have 128 hidden units. The recurrent dropout used is 0.2. The output of both forward and backward layers are concatenated, resulting a vector whose length is 256. We also use dropout layer on top of the concatenated layer by the value of 0.2. The output of the dropout layer is then processed by the next two LSTM layers using the same way as the previous steps. The output of both layers are also concatenated and processed by the dropout layer. The output of the final dropout layer is connected to the time-distributed dense layer with softmax activation function. This last layer produces the labels of semantic roles. Likewise in the previous architectures, the function returns the trained model as well as the prediction result of the test data.

\subsubsection{DBLSTM-Zhou}
Deep BLSTM-Zhou (DBLSTM-Zhou) consists of two pairs of BLSTM-Zhou. Pseudocode~\ref{code:dblstmzhou} shows the implementation of the DBLSTM-Zhou architecture.

\begin{kode}
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	
	\SetKwProg{Fn}{Function}{ is}{end}
	\Fn{blstm(xTrain, yTrain, timesteps, features, xTest, yTest)}{
		\Input{x train, y train, number of time steps, number of features, x test, y test}
		\Output{trained model, testing prediction result}
		\BlankLine
		
		input = Input(shape=(timesteps, features))\;
		\BlankLine
		
		forward1 = LSTM(units=128, returnSequences=True, recurrentDropout=0.2)(input)\;
		dropout1f = Dropout(0.2)(forward1)\;
		backward1 = LSTM(units=128, returnSequences=True, goBackwards=True, recurrentDropout=0.2)(dropout1f)\;
		dropout1b = Dropout(0.2)(backward1)\;
		\BlankLine
		
		forward2 = LSTM(units=128, returnSequences=True, recurrentDropout=0.2)(input)\;
		dropout2f = Dropout(0.2)(forward2)\;
		backward2 = LSTM(units=128, returnSequences=True, goBackwards=True, recurrentDropout=0.2)(dropout2f)\;
		dropout2b = Dropout(0.2)(backward2)\;
		\BlankLine
		
		output = TimeDistributed(Dense(units=17, activation='softmax'))(dropout2b)\;
		\BlankLine
		
		model = Model(inputs=[input], outputs=[output])\;
		model.compile(loss='categoricalCrossentropy', optimizer='adam')\;
		model.fit(xTrain, yTrain, epochs=50, batchSize=150)\;		
		prediction = model.predict(xTest)\;
		\BlankLine
		
		\Return model, prediction\;
	}
	
	\caption{A pseudocode for building and training DBLSTM-Zhou architecture}
	\label{code:dblstmzhou}
\end{kode}

The input layer is first processed by the first forward LSTM layer. The output of this layer is then connected to the first dropout layer. After that, the dropout output is processed by the first backward LSTM layer, followed by the second dropout layer. At this point, one pair of BLSTM-Zhou is built. These processes are done one more time to build the second stack of BLSTM-Zhou. The output of the last layer is then connected to the time-distributed dense layer with softmax activation function. This last layer produces the labels of semantic roles. As in the previous functions, this function also returns the trained model as well as the prediction result of the test data.

\subsubsection{DBLSTM-Highway}
Deep BLSTM-Zhou (DBLSTM-Highway) consists of two pairs of BLSTM-Highway. To implement the DBLSTM-Highway, line 3 to 10 in Pseudocode~\ref{code:dblstmzhou} are changed into the codes presented in Pseudocode~\ref{code:dblstmhighway}.

\begin{kode}
		forward1 = LSTM(units=128, returnSequences=True, recurrentDropout=0.2)(input)\;
		dropout1f = Dropout(0.2)(forward1)\;
		\BlankLine
		
		backward1 = LSTM(units=128, returnSequences=True, goBackwards=True, recurrentDropout=0.2)(dropout1f)\;
		merged1b = Concatenate([backward1, dropout1f])\;
		alpha1b = TimeDistributed(Dense(1, activation='sigmoid'))(merged1b)\;
		out1b = Lambda(timesAlphaSigmoid)([alpha1b, backward1, dropout1f])\;
		dropout1b = Dropout(0.2)(out1b)\;
		\BlankLine
		
		forward2 = LSTM(units=128, returnSequences=True, goBackwards=True, recurrentDropout=0.2)(dropout1b)\;
		merged2f = Concatenate([forward2, backward1])\;
		alpha2f = TimeDistributed(Dense(1, activation='sigmoid'))(merged2f)\;
		out2f = Lambda(timesAlphaSigmoid)([alpha2f, forward2, backward1])\;
		dropout2f = Dropout(0.2)(out2f)\;
		\BlankLine
		
		backward2 = LSTM(units=128, returnSequences=True, goBackwards=True, recurrentDropout=0.2)(dropout2f)\;
		merged2b = Concatenate([backward2, forward2])\;
		alpha2b = TimeDistributed(Dense(1, activation='sigmoid'))(merged2b)\;
		out2b = Lambda(timesAlphaSigmoid
		)([alpha2b, backward2, forward2])\;
		dropout2b = Dropout(0.2)(out2b)\;
	
	\caption{A pseudocode for building and training DBLSTM-Highway architecture}
	\label{code:dblstmhighway}
\end{kode}

The input layer is first processed by the first forward LSTM layer. The output of this layer is then connected to the first dropout layer. After that, the output is processed by the first backward LSTM layer. The output of the first backward layer (\textit{backward1}) and the first dropout layer (\textit{dropout1f}) are concatenated before being processed by the time-distributed dense layer with sigmoid function. The output of the dense layer is the weight \textit{alpha2b}. In lambda function called \textit{timesAlphaSigmoid}, \textit{backward1} is multiplied by the weight \textit{alpha2b} and \textit{dropout1f} is multiplied by the weight \textit{1 - alpha2b}. The lambda function then sums both results. These steps are also done for the next forward and the backward LSTM layers. As in the previous architectures, the output of the last layer is then connected to the time-distributed dense layer with softmax activation function to predict the semantic roles.

\subsection{Additional Layers}
The additional layer options consist of Convolutional Neural Networks (CNN) and attention mechanism. These additional layers can be used in addition to any of the main layers.

\subsubsection{Convolutional Neural Networks (CNN)}
CNN-BLSTM adds a CNN layer underneath the BLSTM layers. Pseudocode~\ref{code:cnnblstm} shows the implementation of the CNN-BLSTM architecture.
\begin{kode}
	inputLayer = Input(shape=(timesteps, features))\;
	cnnLayer = Conv1D(filters=128, kernelSize=3, padding='same', activation='relu', strides=1)(inputLayer)\;
	\caption{A pseudocode for adding CNN layer underneath the main layer}
	\label{code:cnnblstm}
\end{kode}

In this architecture, the CNN layer is added before the BLSTM layers. Thus, the input layer is connected with the CNN layer. The parameters of the CNN layer are filters, kernel size, and strides. Filters represent the number of output hidden layers. Kernel size defines the context window of the CNN layer. Strides define the amount of slide for every time step. The value of filters, kernel size, and strides parameters are 128, 3, and 1, respectively. The output the CNN layer is then connected to the any LSTM variant as explained in the previous sections.

\subsubsection{Attention Mechanism}
The attention mechanism can be added on top of the any main layer variant. Pseudocode~\ref{code:cablstm} shows the implementation of the adding attention mechanism on top of the main layer architecture.
\begin{kode}
	outs1 = TimeDistributed(RawDense(outputDim=512))(dropout)\;
	m = Lambda(sum)(outs1)\;
	alpha = Dense(timesteps, activation="softmax")(m)\;
	z = Lambda(timesAlphaSum)([alpha, dropout])\;
	dropoutZ = Dropout(0.2)(z)\;
	repeatedZ = RepeatVector(timesteps)(dropoutZ)\;
	outFinal = concatenate([dropout, repeatedZ])\;	
	output = TimeDistributed(Dense(units=17, activation='softmax'))(outFinal)\;
	
	\caption{A pseudocode for adding attention mechanism on top of the main layer architecture}
	\label{code:cablstm}
\end{kode}

The output of the dropout layer from last LSTM layer is first fed into a time-distributed, raw dense layer. The output dimension of this dense layer is set to 512. The output of this layer is then summed for each time step. The result of this sum is then fed into the dense layer with softmax activation function. This dense layer outputs the weights alpha for each time step. Lambda function \textit{timesAlphaSum} firstly multiplies the original dropout output of the last LSTM layer with the alpha values. The function then sums all the result element-wise in order to get a context vector z. Vector z is then duplicated by the number of time steps before it is concatenated with all the original dropout output of the last LSTM layer. These concatenated vectors are then fed into the last softmax layer to produce the output labels.

%\section{Experiment}
%
%\section{Evaluation}

%\section{Eksperimen}
%Pada tahap ini \saya~melakukan eksperimen model yang dikembangkan pada tahap sebelumnya. Sebelum masuk ke tahap eksperimen, \saya~melakukan beberapa tahap pra-eksperimen seperti melakukan pemecahan data sebagai implementasi \textit{cross-fold validation}. \Saya~memecah data menjadi 10 bagian dan disimpan dalam sebuah \textit{array} untuk masing-masing fitur. Berikut merupakan \textit{pseudocode} untuk melakukan pemecahan data
%
%\begin{kode}
%
%	\SetKwInOut{Input}{Input}
%	\SetKwInOut{Output}{Output}
%	
%	\SetKwProg{Fn}{Function}{ is}{end}
%	\Fn{splitting(featureArr)}{
%		\Input{array of feature}
%		\Output{splitted array of feature}
%		\BlankLine
%		
%		lenSplit = len(featureArr)/10\;
%		arrSplitted = []\;
%		\For{i=0; i<10;i++}{
%			start = i * lenSplit\;
%			end = (i+1) * lenSplit\;
%			arrSplitted.append[start:end]\;
%		}
%		\BlankLine
%		
%		\Return arrSplitted;
%	}
%	
%	\caption{\textit{Pseudocode} untuk memecah \textit{data} menjadi 10 bagian}	
%	\label{code:split}
%\end{kode}
%
%Setelah masing-masing fitur dipecah menjadi 10 bagian, \saya~melakukan penggabungan antar fitur sebagai \textit{input} untuk melakukan eksperimen. Seperti yang dijelaskan pada tahap sebelumnya, \saya~menggunakan dua arsitektur RNNs. Hasil dari eksperimen tersebut ditulis dalam sebuah berkas dengan format JSON yang nantinya akan menjadi \textit{input} pada tahap evaluasi. Berikut merupakan implementasi eksperimen dengan masing-masing arsitektur tersebut.
%
%\begin{kode}
%	
%	\SetKwInOut{Input}{Input}
%	\SetKwInOut{Output}{Output}
%	
%	\SetKwProg{Fn}{Function}{ is}{end}
%	arrAllFeature = []\;
%	\ForEach{feature in arrSplitted}{
%		arrAllFeature.join(feature)\;
%	}
%	\BlankLine
%	
%	\For{i=0; i<10;i++}{
%		training = arrSplitted[0:i] + arrSplitted[i+1:10]
%		testing = arrSplitted[i]\;
%		
%		\BlankLine
%		result1 = lstm1(training, testing)\;
%		result2 = lstm2(training, testing)\;
%		
%		\BlankLine
%		writeToJSON(result1)\;
%		writeToJSON(result2)\;
%	}
%	\caption{\textit{Pseudocode} untuk melakukan eksperimen}
%	\label{code:eksperimen}	
%\end{kode}
%
%\section{Evaluasi}
%Dalam melakukan implementasi pada tahap evaluasi, \saya~menghitung nilai \textit{prescision, recall} dan \textit{F-Measure} untuk mengukur tingkat keakuratan model yang dikembangkan pada tahap sebelumnya. \Saya~menggunakan aturan yang telah dijelaskan pada Bab 3. Berikut merupakan implementasi kode untuk melakukan evaluasi.
%
%\begin{kode}
%	
%	\SetKwInOut{Input}{Input}
%	\SetKwInOut{Output}{Output}
%	
%	\SetKwProg{Fn}{Function}{ is}{end}
%	resultTag = load(resultRNN)\;
%	originalTag = load(originalTag)\;
%	\BlankLine
%	
%	TP = newHash()\;
%	FP = newHash()\;
%	FN = newHash()\;
%	\For{i = 0; i < len(resultTag); i++}{
%		sentenceResult = resultTag[i]\;
%		sentenceOriginal = originalTag[i]\;
%		\For{j = 0; j < len(sentenceOriginal); i++}{
%			wordResult = sentenceResult[j]\;
%			wordOri = sentenceOriginal[j]\;
%			\uIf{wordOri != O}{
%				\uIf{wordResult != O}{
%					\uIf{wordOri == wordResult}{
%						TP[wordOri] += 1\;
%					}
%					\Else{
%						FN[wordOri] += 1\;
%					}
%				}
%				\Else{
%					FN[wordOri] += 1\;
%				}
%			}
%			\Else{
%				\uIf{wordResult != O}{
%					FP[wordOri] += 1\;
%				}
%			}
%		}
%	}
%	\BlankLine
%
%	prec = newHash()\;
%	rec = newHash()\;
%	fMeas = newHash()\;
%	\ForEach{label in TP}{
%		prec[label] = TP[label] / (TP[label] + FP[label])\;
%		rec[label] = TP[label] / (TP[label] + FN[label])\;
%		fMeas[label] = 2 * 	(prec[label] * rec[label]) / (prec[label] + rec[label])\;
%	}
%	\BlankLine
%	
%	\ForEach{label in prec}{
%		print "Precission", label, prec[label]\;
%		print "Recall", label, rec[label]\;
%		print "F-Measure", label, fmeas[label]\;
%	}
%	\BlankLine
%	\caption{\textit{Pseudocode} untuk melakukan evaluasi}	
%	\label{code:evaluasi}
%\end{kode}