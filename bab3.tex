%!TEX root = skripsi.tex
%-----------------------------------------------------------------------------%
\chapter{\babTiga}\label{bab:tiga}
%-----------------------------------------------------------------------------%
%-----------------------------------------------------------------------------%
% Ide, model matematik, rumus, fitur, jangan m=ngomongin teknologi (java, keras, dll), word2vec boleh %
This chapter describes the methodology used in this research. It consists of data gathering, data pre-processing, data annotation, experiment, and evaluation. Before we describe each part in details, we explain the big picture of the methodology in a form of pipeline.

\section{Pipeline}
The goal of this research is to build a model that predicts the semantic roles of each Indonesian conversational sentence. In other languages such as English, there are already semantic role corpora from which the SRL system is built. Unfortunately, that is not the case for Indonesian, since there is no annotated corpus available yet. We therefore create our own with the annotation guideline crafted for Indonesian conversational language. In this research, we focus on building SRL system for the conversational Indonesian language. 

We view SRL as a sequence labeling problem. Suppose that we have an input of \textit{n} words $w = (w_{1}, w_{2}, ..., w_{n})$, the goal is to find the best label sequence $y = (y_{1}, y_{2}, ..., y_{n})$, with $y_{i}$ representing the semantic roles. The probabilities of the label in each time step $i$ is described as follows.
\begin{equation}
P(y_{i}|w_{i-l}, ..., w_{i+l},y_{i-l}, ..., y_{i+l})
\end{equation}

whereby \textit{l} is a small number. 

In this section, we explain our research methodology including the data gathering, data annotation, data pre-processing, model development, experiment, and evaluation. Figure~\ref{fig:pipeline} shows the pipeline of this research.

This research uses the data from one of Kata.ai's chat bots. Firstly, the data is pre-processed before going into the next steps. After that, the data is then annotated with semantic roles based on the annotation guideline proposed by us for Indonesian conversational language.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{images/pipeline}
	\caption{Methodology Pipeline}
	\label{fig:pipeline}
\end{figure}

The model development step consists of feature extraction and model architecture. The features are Word Embedding, POS Tag, and Neighboring Word Embeddings. The model architecture consists of main and additional layer. The main layer options include vanilla Long Short-Term Memories (LSTM), Bi-Directional LSTM (BLSTM), Deep BLSTM (DBLSTM), DBLSTM-Zhou, and DBLSTM Highway. The additional layer choices include Convolutional Neural Networks (CNN) and our proposed attention mechanism.

There are 2 main scenarios for the experiment. The first scenario aims for finding the best set of features that outputs the best performance. The goal of the second scenario is to find which deep learning model architecture has the best result.

We use 5-fold cross validation for every experiment. Each experiment is evaluated based on precision, recall, and F1 of each semantic roles with partial match approach~\citep{seki2003probabilistic}. The performance of a model is retrieved by averaging precision, recall, and F1 of every semantic role.  We then analyze and explain the results of each experiment scenario.

\section{Data Gathering}
In this research we use real-world data from one of Kata.ai chat bots. We firstly retrieved data consisting of 40.000 instances of text chats. We then manually deleted junk chats which contain, for example, only laugh or greeting. After that, we run a script to delete duplicate chats. The deletion process outputs a clean data with 30.000 instances in total. Finally, we randomly selected 9.000 out of 30.000 instances as the data to be annotated. This data set will be the one which is trained and tested to build the SRL system.

It is worth to note that conversational language has unique characteristics. First, they use slangs and abbreviations. For example, one might use \textit{"gue"} instead of \textit{"aku"} in \textit{"gue beliin kado deh"}. The grammars are often unstructured and thus, one cannot rely on syntactic parsers to build SRL system for conversational language. The sentences are also filled with interjections such as \textit{"haha"} and \textit{"wkwk"}. Lastly, since conversational sentences are really short, averaging around 5-7 words per sentence, it sometimes contains incomplete information. These are the interesting challenges the SRL system should learn and tackle.

\section{Data Pre-Processing}
After the data has been gathered, the next step is to pre-process the data so that it could be fed into the machine learning model later. In this step, each sentence is going through a process called tokenization. Tokenization splits sentence into its individual tokens. Traditionally, one can split sentence by \textit{space}, however, conversational language often contains a concatenated word with symbol such as "makan yuk!!!". It thus needs further tokenization technique, that is, splitting the alphabet with symbol tokens. This way, "makan yuk!!!" will be tokenized as "makan", "yuk", and "!!!". In addition to tokenization by \textit{space}, the rules are listed as follows:
\begin{enumerate}
	\item <alphabet><symbol> to be <alphabet><space><symbol>
	\item <symbol><alphabet> to be <symbol><space> <alphabet>
\end{enumerate}

\section{Data Annotation}
In this work, we create a new set of semantic roles mainly crafted for Indonesian conversational language. The summary of semantic roles proposed with its examples are presented in Table~\ref{tab:semantic_roles}.

\begin{table}
	\centering
	\caption{Set of Semantic Roles for Conversational Language}
	\label{tab:semantic_roles}
	\begin{tabular}{|l|l|}
		\hline
		\multicolumn{1}{|c|}{Semantic Roles} & \multicolumn{1}{c|}{\textit{Example}} \\
		\hline
		\textsc{Agent}		& \underline{Aku} beliin kamu kado\\
		\textsc{Patient}		& Aku beliin kamu \underline{kado}\\
		\textsc{Beneficiary}	& Aku beliin \underline{kamu} kado\\
		\textsc{Greet} 				& Hai \underline{Budi}! Aku beliin kamu kado\\
		\textsc{Modal} 				& Aku \underline{bisa} makan di rumah besok\\
		\textsc{Location} 			& Aku bisa makan di \underline{rumah} besok\\
		\textsc{Time} 				& Aku bisa makan di rumah \underline{besok} \\
		\hline
	\end{tabular}
\end{table}

These semantic roles are mainly inspired by the work of \cite{saeed19972003}, except that \textsc{Agent} and \textsc{Patient} are adapted from \textsc{Proto-Agent} and \textsc{Proto-Patient} as explained by \cite{dowty1991thematic}. The main difference of this set of semantic roles compared to the previous ones is \textsc{Greet}.

The center of all semantic roles is the \predicate. As in English, \predicate~in Indonesian is usually in a form of \textit{verb}, as illustrated by the examples below:
\begin{description}
	\item[$\bullet$] "Kemarin aku \underline{makan} di rumah"
	\item[$\bullet$] "Aku \underline{ada} ujian nih hari Senin"
\end{description}

In Indonesian, however, predicate can also be an adjective. Some examples are presented as follows:
\begin{description}
	\item[$\bullet$] "Kamu \underline{cantik} deh"
	\item[$\bullet$] "Aku lagi \underline{sedih} nih"
\end{description}

In this section, we briefly explain each semantic roles with their respective examples.
\begin{enumerate}
	\item \agent\\
	\agent~is described as initiator of action or capable of volition. It can also be the one which perceives action but not in control. Usually, an \agent~is the subject of a verb in active voice.
	The examples of \agent~in a sentence are given as follows:
	\begin{description}
		\item[$\bullet$] "\underline{Aku} makan ayam dulu ya"
		\item[$\bullet$] "\underline{Kamu} gak tidur?"
		\item[$\bullet$] "\underline{Kamu} mau beliin aku pulsa?"
	\end{description}
	
	\item \patient\\
	\patient~is described as an entity affected by action or undergoes change of state. It can also be an entity being located. \patient~is usually the direct object of a verb in active voice. If the predicate is in a form of adjective, the subject of it is labeled as \patient. The examples of \patient~in a sentence are given as follows:
	\begin{description}
		\item[$\bullet$] "Aku makan \underline{ayam} dulu ya"
		\item[$\bullet$] "Kamu mau beliin aku \underline{pulsa}?"
		\item[$\bullet$] "\underline{Aku} lagi sedih nih.."
	\end{description}

	\item \beneficiary\\
	\beneficiary~is an entity which gets benefit of the predicate. It is usually in a form of indirect object of a predicate (can be a ditransitive verb or an adjective). The examples of \beneficiary~in a sentence are given as follows:
	\begin{description}
		\item[$\bullet$] "Kamu mau beliin \underline{aku} pulsa?"
		\item[$\bullet$] "Aku pengen ngobrol sama \underline{kamu}"
	\end{description}
	
	\item \greet\\
	\greet~is the main difference of this set of semantic roles for conversational language. \greet~refers to an animate object, usually a person, which is being greeted in a chat. In conversational language, one often calls the name of person it is talking to. This information is useful, for instance, we can derive that \textit{"kamu"} refers to \textit{"Budi"} in \textit{"Halo Budi! Aku beliin kamu kado loh"}. The examples of \greet~in a sentence are given as follows:
	\begin{description}
		\item[$\bullet$] "Hai \underline{rizky}! kamu udah makan belum?"
		\item[$\bullet$] "aku ga bisa tidur nih \underline{Val}"
	\end{description}

	\item \modal\\
	\modal~refers to \textit{modal verb} of a predicate. The \modal~examples are \textit{"boleh, harus, pernah, sudah, udah, mesti, perlu, akan, lagi, bisa, mau, ingin, pengen, pingin"}. The examples of \modal~in a sentence are given as follows:
	\begin{description}
		\item[$\bullet$] "Aku \underline{mau} makan dulu ya!"
		\item[$\bullet$] "Kamu \underline{udah} tidur belum?"
	\end{description}

	\item \location\\
	\location~refers to the location of a predicate. The examples of \location~in a sentence are given as follows:
	\begin{description}
		\item[$\bullet$] "Aku mau makan di \underline{rumah} ya!"
		\item[$\bullet$] "Kamu gak pergi ke \underline{sekolah}?"
	\end{description}

	\item \timesrl\\
	\timesrl~ refers to the time of a predicate. The examples of \timesrl~ in a sentence are given as follows:
	\begin{description}
		\item[$\bullet$] "\underline{Kemarin} aku makan di rumah"
		\item[$\bullet$] "Aku ada ujian nih \underline{hari Senin}"
	\end{description}
\end{enumerate}

Following ~\cite{collobert2011natural}, all the labels are tagged using BIO (Begin Inside Outside) tagging. Suppose that a label \patient~consists of more than one word, such as \textit{"ayam goreng"} in "Aku makan \textit{ayam goreng}", "ayam" and "goreng" are tagged as "B-Patient" and "I-Patient", respectively. If the label has only one word, it is then tagged as "B-Patient". Word that does not have any label is thus tagged as "O" which means "Others".

After the data has been labeled, the labels need to be encoded in a way the deep learning model understands. To do so, the labels are then transformed into \textit{one-hot-vector}. Each label is mapped into a unique one-hot-vector, hence the relation is 1-to-1.
\begin{table}
	\centering
	\caption{An example of semantic roles with the respective one-hot-vectors}
	\label{tab:examplelabel}
	\begin{tabular}{|lcccc|}
		\hline
		\textbf{Sentence} 				& Aku & pengen & makan & ayam \\
		\hline
		\textbf{BIO-Label}				& B-AGEN & B-MD & B-PRED & B-PATIENT  \\
		\hline
		\textbf{One-Hot-Vector}		& [1, 0, 0, .., 0] & [0, 1, 0, .., 0] & [0, 0, 1, .., 0] & [0, .., 1, 0, 0] \\
		\hline
	\end{tabular}
\end{table}

Table~\ref{tab:examplelabel} shows an example of how sentence is labeled with the one-hot vectors representations of BIO format.

\section{Feature Extraction}
In this step, we extract features from the data that has been annotated with semantic roles. We propose three features which will be combined later to find the best feature selection that outputs the best result. Those features are word embedding, POS-Tag, and neighboring word embeddings.

\subsection{Word Embedding}
% Apa itu word embedding
Word embedding represents word as a vector. Word embedding has proved to be one of the most contributing features by a lot of deep learning research, such as for SRL system proposed by \cite{zhou2015end} and \cite{collobert2011natural}. The interesting characteristic of word embedding is that similar words have proved to have similar vectors~\citep{mikolov2013efficient}. This is very important when dealing with conversational language which has a lot of slang words. For instance, pronoun "Aku" will have similar vector with its slang form, "Gue". We believe that this feature will contribute greatly to the model performance. Therefore, we utilize word embedding as one of our feature candidates.

% Cara mendapat word embedding model
In order to utilize word embedding as one of our features, we conduct three steps which consist of: 1.) data gathering for building word embedding model, 2.) training the word embedding model, and 3.) converting words into vectors with the trained word embedding model.

\begin{enumerate}
	\item Data gathering for building word embedding model\\
	%Berikan penjelasan
	We first gather an unlabeled dataset in order to build the word embedding model. The dataset is retrieved from 1.300.000 sentences of chats from Kata.ai.
	
	\item Training the word embedding model\\
	%Berikan penjelasan
	The word embedding model is trained using the afore-mentioned unlabeled data set. This model is used to transform words into their respective vector representations.
	
	\item Converting words into vectors with the trained word embedding model\\
	%Berikan penjelasan
	The trained word embedding model is then used to convert words into vectors. The example of this conversion can be seen in Table~\ref{tab:examplewe}
\end{enumerate}

\begin{table}
	\centering
	\caption{An example of word embedding vector representation with dimension of 3}
	\label{tab:examplewe}
	\begin{tabular}{|lcccc|}
		\hline
		\textbf{Sentence} 				& Aku & pengen & makan & ayam \\
		\hline
		\textbf{Word Vector}		& [0.2, -0.4, 0.9] & [0.7, 0.1, 0.2] & [0.1, 0.6, -0.5] & [0.9, 0.1, 0.8] \\
		\hline
	\end{tabular}
\end{table}

In Table~\ref{tab:examplewe} provides an example assuming that the vector dimension is 3. This means that every word is mapped into a vector with a length of 3.

\subsection{Part-of-Speech Tag (POS Tag)}
POS Tag is a feature to represent the class of each word. In this research, the POS tags used are: Verb (V), Noun (NN), Adjective (ADJ), Adverb (ADV), Coordinative Conjunction (CC), Subordinative Conjunction (SC), Interjection (INTJ), Question (WH), Preposition (PREP), and Negation (NEG). Before feeding the feature to the deep learning model, we encode the POS tag as a one-hot-vector. Each one-hot-vector represents each POS tag uniquely.

The example of POS Tag features is presented in Table~\ref{tab:examplepos}
\begin{table}
	\centering
	\caption{An example of POS Tag feature and its respective one-hot-vector}
	\label{tab:examplepos}
	\begin{tabular}{|lcccc|}
		\hline
		\textbf{Sentence} 				& Aku & pengen & makan & ayam \\
		\hline
		\textbf{POS Tag}				& NN & ADV & V & NN  \\
		\hline
		\textbf{One-Hot-Vector}		& [1, 0, 0, .., 0] & [0, 1, 0, .., 0] & [0, 0, 1, .., 0] & [0, .., 1, 0, 0] \\
		\hline
	\end{tabular}
\end{table}

While most of the deep learning research aims for not using such feature, we argue that POS tag is still important for building a robust model in our case. This is because of the fact that size of our corpus is relatively small compared to the huge English-based SRL corpus such as ConLL 2005 by~\cite{carreras2005introduction}. We argue that having Verb as one of our POS Tag will help to determine which one is the predicate, since predicate is usually in a form of verb, though some can also be adjective. As the arguments having semantic role are mostly in a form of Noun, POS Tag Noun will be a helpful information as well. Other POS tags will help to determine which word that obviously does not have any semantic role.

In this work, we use gold-standard POS tag on our data as the features to prevent propagation errors from the POS tag model. This way, we can focus on analyzing errors resulting from the SRL model later in chapter 5. 

\subsection{Neighboring Word Embeddings}
Neighboring word embeddings are the vector representations of words located before and after the word being processed. We use specifically one word before and after the word being processed. Suppose that we are processing the word $w_{t}$ at time step $t$, the neighboring words embeddings are the vector representations of the word $w_{t-1}$ and $w_{t+1}$. We argue that this feature can be useful for capturing the context of the word by looking at the surrounding words. Suppose that the machine is processing the word \textit{"rumah"} in \textit{"aku tidur di rumah"}. By looking at the previous word, which is the preposition \textit{"di"}, it gives a hint that \textit{"rumah"} might have the semantic role \location.

\begin{table}
	\centering
	\caption{An example of neighboring word embedding vectors of every time step}
	\label{tab:examplenwe}
	\begin{tabular}{|lcccc|}
		\hline
		\textbf{Sentence} 				& Aku & pengen & makan & ayam \\
		\hline
		\textbf{Word Vector}		& [0.2, -0.4, 0.9] & [0.7, 0.1, 0.2] & [0.1, 0.6, -0.5] & [0.9, 0.1, 0.8] \\			\textbf{Neighbor Vector}	& [0.0, 0.0, 0.0] & [0.2, -0.4, 0.9] & [0.7, 0.1, 0.2] & [0.1, 0.6, -0.5] \\
													& [0.7, 0.1, 0.2] & [0.1, 0.6, -0.5] &  [0.9, 0.1, 0.8] &  [0.0, 0.0, 0.0] \\
		\hline
	\end{tabular}
\end{table}

Table~\ref{tab:examplenwe} illustrates the neighboring word embeddings of every time step. The table shows that in every time step, it adds the word vector information from the left and right. Since the first word does not have any previous word, its left neighbor is a vector $\vec{0}$. This is also the case for the last word which does not have any subsequent word. Hence, the right neighbor of the last word is also a vector  $\vec{0}$.

\section{Model Architecture}
Recurrent Neural Networks (RNN) has a nature advantage for solving sequence labeling problem~\citep{zhou2015end}. ~\cite{hochreiter1997long} proposed Long Short-Term Memories (LSTM) as the specific version of RNN designed to overcome vanishing and exploding gradient problem. In this research, we experiment on various LSTM architectures, namely vanilla LSTM, Bi-Directional LSTM (BLSTM), Deep BLSTM (DBLSTM), DBLSTM-Zhou, and DBLSTM-Highway. All these LSTM architectures are considered as the main layers. Moreover, there are two additional layers: Convolutional Neural Networks (CNN) and attention. CNN layer is located underneath the main layer while the attention is placed on top of the main layer. The attention layer is the one we proposed in this research. These additional layers can be used for any afore-mentioned LSTM architectures.

\subsection{Main Layers}
The main layers consist of Vanilla LSTM (LSTM), Bi-Directional LSTM (BLSTM), Deep BLSTM (DBLSTM), DBLSTM-Zhou, and DBLSTM-Highway.

\subsubsection{Vanilla LSTM (LSTM)}
% Explain what is LSTM (generally)
Vanilla LSTM is the basic, one-directional LSTM networks designed to overcome vanishing and exploding gradient problem found in RNN. To do so, it has forget gates, a gate to open or close incoming information from the previous time steps. We firstly explain the LSTM networks as the big picture for solving sequence labeling problem, followed by the details of every LSTM unit. 

% Gambar Big Picture LSTM
\begin{figure}
	\centering
	\includegraphics[width=0.75\linewidth]{images/olstm}
	\caption{An architecture of vanilla LSTM with total time step of 4}
	\label{fig:olstm}
\end{figure}

% Explain the formula based on big picture LSTM
Figure~\ref{fig:olstm} illustrates the big picture of the vanilla LSTM networks used in this research. Suppose that we have sequence of input vectors $[\mathbf{x_{1}}; \mathbf{x_{2}}; ...; \mathbf{x_{n}}]$, with $n$ denotes the number of time steps. Each input vector $\mathbf{x_{t}}$ represents features of the word in time step $t$. Suppose that we have more than one feature (for example: Word Embedding and POS Tag), the vectors of these features are concatenated. For every time step $t$, each input vector $\mathbf{x_{t}}$ is fed into the LSTM layer, resulting vector $\mathbf{f_{t}}$, as shown in Equation~\ref{eq:lstm}. 

\begin{equation}
\label{eq:lstm}
\mathbf{f_{t}} = LSTM(\mathbf{x_{t}}, \mathbf{f_{t-1}})
\end{equation}

Equation~\ref{eq:softmaxout} shows that each LSTM output from every time step is then processed by the time-distributed softmax layer to produce the probabilities of every possible label.

\begin{equation}
\label{eq:softmaxout}
label_{t} = Softmax(\mathbf{f_{t}})
\end{equation}

In each time step, label with highest probability among others will be the final output. This way, we have all the predicted labels for every time step.

After explaining the big picture of LSTM networks, we describe the details of every LSTM unit in time step $t$ shown in Equation~\ref{eq:lstm}. That is, given an input vector $\mathbf{x_{t}}$, each LSTM unit will output vector $\mathbf{f_{t}}$. Figure~\ref{fig:lstmunit} illustrates the architecture of one LSTM unit in time step $t$.

% Gambar single unit LSTM
\begin{figure}
	\centering
	\includegraphics[width=0.85\linewidth]{images/lstmadapted2}
	\caption{An LSTM unit in time step $t$. Adapted from~\citep{skripsiwahid}.}
	\label{fig:lstmunit}
\end{figure}

% Explain the formula based on single cell LSTM
The LSTM unit in time step $t$ requires an input vector $\mathbf{x_{t}}$. The equations to produce output vector $\mathbf{f_{t}}$ are presented as follows:

\begin{equation}\label{eq:lstmm}
\mathbf{m_{t}}=\alpha_{t}\cdot~\mathbf{m_{t-1}} + \beta_{t}\cdot~tanh(W_{xm} \cdot \mathbf{x_{t}} + W_{fm} \cdot \mathbf{f_{t-1}})
\end{equation}
\begin{equation}\label{eq:lstmh}
\mathbf{f_{t}}=\gamma_{t}\cdot~tanh(\mathbf{m_{t}})
\end{equation}
where
$ \alpha_t $, $ \beta_t $, and $ \gamma_t $ are the gates:
\begin{enumerate}
	\item \textit{Forget gates}: $ \alpha_{t}=\sigma(W_{x\alpha} \cdot \mathbf{x_{t}}+W_{f\alpha}\cdot~\mathbf{f_{t-1}}+W_{m\alpha}\cdot~\mathbf{m_{t-1}}) $
	\item \textit{Input gates}: $ \beta_{t}=\sigma(W_{x\beta}\cdot \mathbf{x_{t}}+W_{f\beta}\cdot~\mathbf{f_{t-1}}+W_{m\beta}\cdot~\mathbf{m_{t-1}}) $
	\item \textit{Output gates}: $ \gamma_{t}=\sigma(W_{x\gamma}\cdot \mathbf{x_{t}}+W_{f\gamma}\cdot~\mathbf{f_{t-1}}+W_{m\gamma}\cdot~\mathbf{m_{t-1}}) $
\end{enumerate}

It is worth noting that the LSTM layer is recursive, meaning that one of the inputs comes from the output of previous time step. This way, the result of each time step also depends on the previous ones.


\subsubsection{Bi-Directional LSTM (BLSTM)}
\label{sec:blstm}
% What is Bi-Directional LSTM generally, what it's for. Menurut Zhou gimana
Bi-Directional LSTM (BLSTM) is a modification of LSTM networks. It was firstly introduced by \cite{schuster1997bidirectional} for the original recurrent neural networks. While vanilla LSTM only goes on one direction, BLSTM goes both ways in order to capture context information from the past and future, as explained by~\cite{zhou2015end}. The idea is to have two LSTM layers, one for going forward and another for going backward, as illustrated in Figure~\ref{fig:bilstm}.

% Gambar BILSTM
\begin{figure}
	\centering
	\includegraphics[width=0.75\linewidth]{images/blstm}
	\caption{An architecture of Bi-Directional LSTM (BLSTM) with total time step of 4}
	\label{fig:bilstm}
\end{figure}

% Explain the formula based on big picture BILSTM
Figure~\ref{fig:bilstm} shows that the input $\mathbf{x_{t}}$ in every time step $t$ is fed into two LSTM layers, the first one for going forward and the second one for going backward. These are illustrated in Equation~\ref{eq:lstmforward} and Equation~\ref{eq:lstmbackward}
\begin{equation}
\label{eq:lstmforward}
\mathbf{f_{t}} = ForwardLSTM(\mathbf{x_{t}}, \mathbf{f_{t-1}})
\end{equation}
\begin{equation}
\label{eq:lstmbackward}
\mathbf{b_{t}} = BackwardLSTM(\mathbf{x_{t}}, \mathbf{b_{t+1}})
\end{equation}

In each time step, the result vectors $\mathbf{f_{t}}$ and $\mathbf{b_{t}}$ are then concatenated to be one vector output $\mathbf{h_{t}}$, as shown in Equation~\ref{eq:lstm_concat}
\begin{equation}
\label{eq:lstm_concat}
\mathbf{h_{t}} = Concatenate(\mathbf{f_{t}}, \mathbf{b_{t}})
\end{equation}

Likewise in vanilla LSTM architecture, the output vector $\mathbf{h_{t}}$ is then fed into softmax layer, as shown in Equation~\ref{eq:bilstm_softmax}

\begin{equation}
\label{eq:bilstm_softmax}
label_{t} = Softmax(\mathbf{h_{t}})
\end{equation}

After which, the output of the softmax layer will determine the final label for each time step.

\subsubsection{Deep BLSTM (DBLSTM)}
Deep BLSTM (DBLSTM) is basically formed by stacking BLSTM layers. This concept was introduced by~\cite{zhou2015end}. However, they proposed a different way of building the BLSTM layer, which will be explained in the next section. This section explains the stacked version of the original BLSTM layers as explained in the previous section. In this work, we stack two BLSTM layers. Figure~\ref{fig:dblstm} illustrates the architecture of DBLSTM.

% Gambar dblstm
\begin{figure}
	\centering
	\includegraphics[width=0.75\linewidth]{images/dblstm}
	\caption{An architecture of Deep BLSTM (DBLSTM) with total time step of 4}
	\label{fig:dblstm}
\end{figure}

Initially, the input vectors $\mathbf{x_{t}}$ are processed by the first BLSTM layer, resulting the output vectors $\mathbf{h_{t}}$ as explained in Equation~\ref{eq:lstmforward},~\ref{eq:lstmbackward}, and~\ref{eq:lstm_concat}. The output vectors $\mathbf{h_{t}}$ are then processed by the second BLSTM layer. These are illustrated in Equation~\ref{eq:lstmforward2} and Equation~\ref{eq:lstmbackward2}.
\begin{equation}
\label{eq:lstmforward2}
\mathbf{k_{t}} = ForwardLSTM2(\mathbf{h_{t}}, \mathbf{k_{t-1}})
\end{equation}
\begin{equation}
\label{eq:lstmbackward2}
\mathbf{m_{t}} = BackwardLSTM2(\mathbf{h_{t}}, \mathbf{m_{t+1}})
\end{equation}

In each time step, the result vectors $\mathbf{k_{t}}$ and $\mathbf{m_{t}}$ are then concatenated to be one vector output $\mathbf{p_{t}}$, as shown in Equation~\ref{eq:lstm_concat2}
\begin{equation}
\label{eq:lstm_concat2}
\mathbf{p_{t}} = Concatenate(\mathbf{k_{t}}, \mathbf{m_{t}})
\end{equation}

The output vector $\mathbf{p_{t}}$ is finally fed into the last softmax layer to output the final label of each time step $t$.

\subsubsection{Deep BLSTM-Zhou (DBLSTM-Zhou)}
\cite{zhou2015end} proposed another way of constructing the BLSTM layer. First, an LSTM layer processes the input sequence in a forward direction. The output of this layer is then fed into the next LSTM layer as input, processed in backward direction. At this point, one BLSTM-Zhou layer is built. \cite{zhou2015end} then stacked the BLSTM layers and named it as the deep bi-directional LSTM. In this work, we stack two BLSTM-Zhou layers for constructing the DBLSTM-Zhou as presented in Figure~\ref{fig:dblstmzhou}.

\begin{figure}
	\centering
	\includegraphics[width=0.75\linewidth]{images/dblstmzhou}
	\caption{An architecture of DBLSTM-Zhou with total time step of 4}
	\label{fig:dblstmzhou}
\end{figure}

First, the input vector $\mathbf{x_{t}}$ is processed by the first forward LSTM layer, resulting output vector $\mathbf{f_{t}}$. This output is then fed into the backward LSTM layer to produce the output vector $\mathbf{b_{t}}$. These processes are presented in Equation~\ref{eq:lstmforwardzhou1} and~\ref{eq:lstmbackwardzhou1}.
\begin{equation}
\label{eq:lstmforwardzhou1}
\mathbf{f_{t}} = ForwardLSTM1(\mathbf{x_{t}}, \mathbf{f_{t-1}})
\end{equation}
\begin{equation}
\label{eq:lstmbackwardzhou1}
\mathbf{b_{t}} = BackwardLSTM1(\mathbf{f_{t}}, \mathbf{b_{t+1}})
\end{equation}

The output vector $\mathbf{b_{t}}$ is then fed into the next forward LSTM layer to produce the next output vector $\mathbf{k_{t}}$ (Equation~\ref{eq:lstmforwardzhou2}). Finally, $\mathbf{k_{t}}$ is processed by the second backward LSTM layer to produce the output vector $\mathbf{m_{t}}$ (Equation~\ref{eq:lstmbackwardzhou2}).
\begin{equation}
\label{eq:lstmforwardzhou2}
\mathbf{k_{t}} = ForwardLSTM2(\mathbf{b_{t}}, \mathbf{k_{t-1}})
\end{equation}
\begin{equation}
\label{eq:lstmbackwardzhou2}
\mathbf{m_{t}} = BackwardLSTM2(\mathbf{k_{t}}, \mathbf{m_{t+1}})
\end{equation}

The output vector $\mathbf{m_{t}}$ is then processed by the softmax layer to produce the final label of every time step $t$.

\subsubsection{Deep BLSTM-Highway (DBLSTM-Highway)}
The DBLSTM-Highway architecture is adapted from the work of~\cite{he2017deep}. They used the same BLSTM architecture as~\cite{zhou2015end} but added the so-called highway connections~\citep{srivastava2015training} for their DBLSTM architecture. Figure~\ref{fig:dblstmhighway} illustrates the DBLSTM-Highway architecture.

\begin{figure}
	\centering
	\includegraphics[width=0.75\linewidth]{images/dblstmhighway}
	\caption{An architecture of DBLSTM-Highway with total time step of 4}
	\label{fig:dblstmhighway}
\end{figure}

First, the input vector $\mathbf{x_{t}}$ is processed by the first forward LSTM layer which produces output vector $\mathbf{f_{t}}$.
\begin{equation}
\label{eq:forwardhighway1}
\mathbf{f_{t}} = ForwardLSTM1(\mathbf{x_{t}}, \mathbf{f_{t-1}})
\end{equation}

The vector $\mathbf{f_{t}}$ is then fed into the first backward LSTM layer which outputs vector $\mathbf{b_{t}}$. The vectors $\mathbf{b_{t}}$ and $\mathbf{f_{t}}$ are then concatenated before being processed by the sigmoid function which outputs the weight $s_{1,t}$. After that, $\mathbf{b'_{t}}$ is retrieved by summing the multiplication results of $s_{1,t}$ with $\mathbf{b_{t}}$ and $(1 - s_{1,t})$ with $\mathbf{f_{t}}$

\begin{equation}
\label{eq:backwardhighway1}
\mathbf{b_{t}} = BackwardLSTM1(\mathbf{f_{t}}, \mathbf{b_{t+1}})
\end{equation}
\begin{equation}
	s_{1,t} = \sigma(W_{1} \cdot [\mathbf{b_{t}}, \mathbf{f_{t}}] + d_{1})
\end{equation}
\begin{equation}
	\mathbf{b'_{t}} = s_{1,t} \cdot \mathbf{b_{t}} + (1 - s_{1,t}) \cdot \mathbf{f_{t}} 
\end{equation}

$\mathbf{k'_{t}}$ is retrieved by the same way as $\mathbf{b'_{t}}$, which we define as follows:
\begin{equation}
\label{eq:forwardhighway2}
\mathbf{k_{t}} = ForwardLSTM2(\mathbf{b'_{t}}, \mathbf{k_{t-1}})
\end{equation}
\begin{equation}
s_{2,t} = \sigma(W_{2} \cdot [\mathbf{k_{t}}, \mathbf{b_{t}}] + d_{2})
\end{equation}
\begin{equation}
\mathbf{k'_{t}} = s_{2,t} \cdot \mathbf{k_{t}} + (1 - s_{2,t}) \cdot \mathbf{b_{t}} 
\end{equation}

Lastly, $\mathbf{m'_{t}}$ is retrieved by the same way as $\mathbf{k'_{t}}$, which is defined as follows:
\begin{equation}
\label{eq:backwardhighway2}
\mathbf{m_{t}} = BackwardLSTM2(\mathbf{k'_{t}}, \mathbf{m_{t+1}})
\end{equation}
\begin{equation}
s_{3,t} = \sigma(W_{3} \cdot [\mathbf{m_{t}}, \mathbf{k_{t}}] + d_{3})
\end{equation}
\begin{equation}
\mathbf{m'_{t}} = s_{3,t} \cdot \mathbf{m_{t}} + (1 - s_{3,t}) \cdot \mathbf{k_{t}} 
\end{equation}

The output vector $\mathbf{m'_{t}}$ is then processed by the softmax layer to produce the final label of every time step $t$.

\subsection{Additional Layers}
The additional layers consist of Convolutional Neural Networks (CNN) and attention mechanism. These additional layers can be used in addition to any of the main layers.

\subsubsection{Convolutional Neural Networks (CNN)}
% What is CNN-BLSTM generally, what it's for.
In addition to the main layer architecture, we also experiment on adding Convolutional Neural Networks (CNN) layer underneath the main layer. The rationale is to capture raw context from the neighboring input vectors. This way, CNN can implicitly extract meaningful context information. Figure ~\ref{fig:cnndblstmhighway} illustrates the big picture of adding CNN underneath the main layer. The main layer is illustrated by the shaded architecture inside the rectangle. The main layer can be changed into any LSTM variants explained in the previous sections. In this illustration, we use DBLSTM-Highway as the main layer.

% Gambar CNN-BLSTM
\begin{figure}
	\centering
	\includegraphics[width=0.75\linewidth]{images/cnndblstmhighway}
	\caption{An architecture of adding CNN underneath the main layer with total time step of 4. The main layer is illustrated by the shaded architecture inside the rectangle. The main layer can be changed into any LSTM variants.}
	\label{fig:cnndblstmhighway}
\end{figure}

% Explain the formula based on big picture CNN-BILSTM
Instead of feeding the main layer with raw input vector $\mathbf{x_{t}}$, this architecture firstly processes it through CNN layer, resulting output vector $\mathbf{c_{t}}$, as shown in Equation~\ref{eq:cnn}.
\begin{equation}
\label{eq:cnn}
[\mathbf{c_{1}}; \mathbf{c_{2}}; ...; \mathbf{c_{n}}] = CNN([\mathbf{x_{1}}; \mathbf{x_{2}}; ...; \mathbf{x_{n}}])
\end{equation}

The result vector $\mathbf{c_{t}}$ is then fed into the main layer that can be any of LSTM variants. The output of the LSTM layer is then processed by the time-distributed softmax layer to determine the final output label.

\subsubsection{Attention Mechanism}
% What is CA-BLSTM generally, what it's for
In this work, we propose an additional attention mechanism that can be used on top the main layer. The rationale is to add a dense yet useful high-level information containing a sentence context to every time step in order to help the machine to decide semantic roles better. With this in mind, we design an attention mechanism on top of the main layer which can be any of the LSTM variant explained in the previous sections. Figure~\ref{fig:dblstmhighwayattention} illustrates the architecture in which attention mechanism is added on top of the main layer. For this illustration, the DBLSTM-Highway is used as the main layer.

\begin{figure}
	\centering
	\includegraphics[width=0.75\linewidth]{images/BLSTMAttentionNew}
	\caption{An architecture of adding attention mechanism on top of the main layer with total time step of 4. The main layer is illustrated by the shaded architecture inside the rectangle. The main layer can be changed into any LSTM variant.}
	\label{fig:dblstmhighwayattention}
\end{figure}

% Explain the formula based on big picture CA-BLSTM
The attention mechanism firstly collects the context information by multiplying trainable weights with all the vectors from every time step of the last LSTM output. We sum each element for each weighted vectors to reduce the dimension. The results are then fed into a hidden softmax layer which outputs weights with a total of 1. The original output vectors of the last LSTM output are multiplied by these distributed weights respectively. We then sum all the multiplication results as the final context information. The original LSTM outputs are concatenated with this context information before going to the last softmax layer to predict the semantic roles. 

Suppose that we have $\mathbf{s_{i}}$ as the last LSTM output, it is then fed into a differentiable neural networks function $g(\mathbf{s_{i}})$ in which it is multiplied by the time-distributed matrix $\textbf{W} \in {\rm I\!R^{H \times K}}$ and all the elements in it are summed. $H$ is the vector dimension of $\mathbf{s_{i}}$, meanwhile $K$ is the dimension size that we want as an output when we multiply \textbf{$W$} with $\mathbf{s_{i}}$.

\begin{equation}
\label{sum_weight}
g(\mathbf{s_{i}}) = Sum(\mathbf{W}.\mathbf{s_{i}})
\end{equation}

Once we have all the values of $[g(\mathbf{s_{1}}); g(\mathbf{s_{2}}); ...; g(\mathbf{s_{n}})]$, we make it as an input for the softmax layer, resulting weights $[\alpha_{1}; \alpha_{2}; ...; \alpha_{n}]$. All the original LSTM outputs $[\mathbf{s_{1}}; \mathbf{s_{2}}; ...; \mathbf{s_{n}}]$ are multiplied by these weights, with the results of $[\mathbf{r_{1}}; \mathbf{r_{2}}; ...; \mathbf{r_{n}}]$

\begin{equation}
[\alpha_{1}; \alpha_{2}; ...; \alpha_{n}] = Softmax([g(\mathbf{s_{1}}); g(\mathbf{s_{2}}); ..., g(\mathbf{s_{n}})])
\end{equation}
\begin{equation}
\mathbf{r_{i}} = \alpha_{i}.\mathbf{s_{i}}
\end{equation}

We then sum all these vectors element-wise to have a context vector $\mathbf{z}$. 
All the original LSTM outputs are thus concatenated with vector $\mathbf{z}$ as the additional information to predict the semantic roles.
\begin{equation}
\mathbf{z} = \mathbf{r_{1}} + \mathbf{r_{2}} + ... + \mathbf{r_{n}}
\end{equation}
\begin{equation}
\mathbf{j_{i}} = Concatenate(\mathbf{s_{i}}, \mathbf{z})
\end{equation}

Lastly, the time-distributed softmax layer produces the final semantic roles label.

\section{Experiment}
We use 5-fold cross validation for out experiments and thus, the data set is firstly split into 5 sets. These 5 sets are divided into training and testing sets with the ratio of 4:1. After that, we train the model using the training set and evaluate it using the testing set. This process is done 5 times until every set of data is tested.

\section{Evaluation}
In each scenario, we evaluate the trained model in order to see how good it predicts the semantic roles as expected. The metrics for our evaluation are precision, recall, and F1. These metrics are applied to all semantic role labels. We then average each metrics from all semantic role labels to get the average precision, recall, and F1 of a model. The evaluation approach used is partial match in which a set of predicted labels is counted right if there is an intersection with the gold-standard \citep{seki2003probabilistic}. 

Suppose that we are evaluating the semantic role \patient. The rules for evaluation using partial match for semantic role \patient are explained as follows.

\begin{enumerate}
	\item Counting \textit{True Positive} (TP)\\
	For every gold standard label that has intersection with the predicted label, the value of True Positive ($TP$) is added by 1.
	\\
	
	\fbox{%
		\parbox{1.0\linewidth}{%
			Expected: Aku pengen makan <Patient>\textbf{ayam goreng}</Patient> deh\\
			Predicted.1: Aku pengen makan <Patient>\textbf{ayam goreng}</Patient> deh\\
			Predicted.2: Aku pengen makan <Patient>\textbf{ayam}</Patient> goreng deh\\
			Predicted.3: Aku pengen makan <Patient>\textbf{ayam goreng deh} </Patient>\\
			Predicted.4: Aku pengen makan ayam goreng deh
		}%
	}
	\\
	
	The examples above illustrate four scenarios of possible predicted results, denoted as Predicted.1, Predicted.2, Predicted.3, and Predicted.4, given a gold-standard called Expected which has "ayam goreng" as the \patient.
	\\
	
	Predicted.1 predicts exactly the same as the Expected, hence the value of $TP$ is added by 1. The result of Predicted.2 has an intersection with the gold-standard, which is "ayam", the value of $TP$ is then added by 1 as well. Although Predicted.3 predicts too much as it includes "deh" as part of \patient, it also has an intersection with the gold-standard, which is "ayam goreng". The Predicted.3 therefore also adds the value of $TP$ by 1. Meanwhile, Predicted.4 does not predict anything. In this case, the value of $TP$ is not added at all.
	\\
	
	\item Counting \textit{False Positive} (FP)\\
	For every predicted label that should not be predicted according to gold-standard, the value of False Positive ($FP$) is added by 1.
	\\
	
	\fbox{%
		\parbox{1.0\linewidth}{%
			Expected: Aku pengen makan <Patient>\textbf{ayam goreng}</Patient> deh\\
			Predicted.1: <Patient>Aku</Patient> pengen makan ayam goreng deh\\
		}%
	}
	\\
	
	From the example above, "Aku" is predicted as \patient, while it should not be predicted as \patient~according to the gold-standard. This will add the value of $FP$ by 1.
	\\
	
	\item Counting \textit{False Negative} (FN)\\
	For every gold-standard label that is either not predicted or predicted with wrong label, the value of False Negative ($FN$) is added by 1.
	\\
	
	\fbox{%
		\parbox{1.0\linewidth}{%
			Expected: Aku pengen makan <Patient>\textbf{ayam goreng}</Patient> deh\\
			Predicted.1: Aku pengen makan <Agent>\textbf{ayam goreng}</Agent> deh\\
			Predicted.2: Aku pengen makan ayam goreng deh
		}%
	}
	\\
	
	From the example above, Predicted.1 predicts "ayam goreng" as \agent, while it should be predicted as \patient. In this case, the value of $FP$ is added by 1. Predicted.2 illustrates an example which does not predict "ayam goreng" with any label, while it should be predicted as \patient. In this case, the value of $FP$ is added by 1 as well.
	
\end{enumerate}

After we have the value of $TP$, $FP$, and $FN$ for the semantic role \patient, we then count the precision, recall, and F1 with following equations:
\begin{align}
Precision &= \frac{TP}{TP+FP}\\
Recall &= \frac{TP}{TP+FN}\\
F1 &= 2 \cdot \frac{Precission \cdot Recall}{Precission + Recall}
\end{align}

Other semantic roles, such as \agent~and \beneficiary, are also evaluated by following these rules. After we have the value of precision, recall, and F1 for every semantic role, we average them to get the average precision, recall, and F1 for the model being evaluated.
	